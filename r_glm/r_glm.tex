% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={HW 12 regression in r},
  pdfauthor={Alumno: Daniel Nuño, daniel.nuno@iteso.mx},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{HW 12 regression in r}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Problem 2}
\author{Alumno: Daniel Nuño,
\href{mailto:daniel.nuno@iteso.mx}{\nolinkurl{daniel.nuno@iteso.mx}}}
\date{4/18/2022}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{problem-2-application-problems}{%
\section{Problem 2: Application
Problems}\label{problem-2-application-problems}}

Note that some details are missing for all the following examples, the
problems lack a complete explanation, and the code may need adequate
comments. In this form, you must present a proper mathematical
formulation, a brief background of the problem (and its bibliographical
references) and, a much better explanation.

\begin{itemize}
\tightlist
\item
  The olsrr packeage

  \begin{enumerate}
  \def\labelenumi{(\alph{enumi})}
  \tightlist
  \item
    \href{https://olsrr.rsquaredacademy.com/articles/intro.html}{Introduction
    to olsrr}
  \item
    \href{https://olsrr.rsquaredacademy.com/articles/variable_selection.html}{Variable
    Selection Methods}
  \item
    \href{https://olsrr.rsquaredacademy.com/articles/residual_diagnostics.html}{Residual
    Diagnostics}
  \item
    \href{https://olsrr.rsquaredacademy.com/articles/heteroskedasticity.html}{Heteroscedasticity}
  \item
    \href{https://olsrr.rsquaredacademy.com/articles/influence_measures.html}{Measures
    of Influence}
  \item
    \href{https://olsrr.rsquaredacademy.com/articles/regression_diagnostics.html}{Collinearity
    Diagnostics, Model Fit \& Variable Contribution}
  \end{enumerate}
\item
  The blorr package

  \begin{enumerate}
  \def\labelenumi{(\alph{enumi})}
  \tightlist
  \item
    \href{https://blorr.rsquaredacademy.com/articles/introduction.html}{A
    Short Introduction to the blorr Package}
  \end{enumerate}
\end{itemize}

\hypertarget{introduction-to-olsrr}{%
\subsection{Introduction to olsrr}\label{introduction-to-olsrr}}

This document is a quick start guide to the tools offered by olsrr.
Other vignettes provide more details on specific topics: - Residual
Diagnostics: Includes plots to examine residuals to validate OLS
assumptions - Variable selection: Different variable selection
procedures such as all possible regression, best subset regression,
stepwise regression, stepwise forward regression and stepwise backward
regression - Heteroskedasticity: Tests for heteroskedasticity include
bartlett test, breusch pagan test, score test and f test - Measures of
influence: Includes 10 different plots to detect and identify
influential observations - Collinearity diagnostics: VIF, Tolerance and
condition indices to detect collinearity and plots for assessing mode
fit and contributions of variables

This example uses \textbf{mtcars} dataset. This dataset contains a
subset of the fuel economy data that the EPA makes available on
\url{https://fueleconomy.gov/}. It contains only models which had a new
release every year between 1999 and 2008 - this was used as a proxy for
the popularity of the car.

A data frame with 234 rows and 11 variables:

\begin{itemize}
\tightlist
\item
  manufacturer: manufacturer name
\item
  model: model name
\item
  displ: engine displacement, in litres
\item
  year: year of manufacture
\item
  cyl: number of cylinders
\item
  trans: type of transmission
\item
  drv: the type of drive train, where f = front-wheel drive, r = rear
  wheel drive, 4 = 4wd
\item
  cty: city miles per gallon
\item
  hwy: highway miles per gallon
\item
  fl: fuel type
\item
  class: ``type'' of car
\end{itemize}

\hypertarget{regression}{%
\subsubsection{Regression}\label{regression}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(olsrr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'olsrr'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:datasets':
## 
##     rivers
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ols\_regress}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                          Model Summary                          
## ---------------------------------------------------------------
## R                       0.914       RMSE                 2.409 
## R-Squared               0.835       MSE                  6.875 
## Adj. R-Squared          0.811       Coef. Var           13.051 
## Pred R-Squared          0.771       AIC                159.070 
## MAE                     1.858       SBC                167.864 
## ---------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                ANOVA                                 
## --------------------------------------------------------------------
##                 Sum of                                              
##                Squares        DF    Mean Square      F         Sig. 
## --------------------------------------------------------------------
## Regression     940.412         4        235.103    34.195    0.0000 
## Residual       185.635        27          6.875                     
## Total         1126.047        31                                    
## --------------------------------------------------------------------
## 
##                                   Parameter Estimates                                    
## ----------------------------------------------------------------------------------------
##       model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper 
## ----------------------------------------------------------------------------------------
## (Intercept)    27.330         8.639                  3.164    0.004     9.604    45.055 
##        disp     0.003         0.011        0.055     0.248    0.806    -0.019     0.025 
##          hp    -0.019         0.016       -0.212    -1.196    0.242    -0.051     0.013 
##          wt    -4.609         1.266       -0.748    -3.641    0.001    -7.206    -2.012 
##        qsec     0.544         0.466        0.161     1.166    0.254    -0.413     1.501 
## ----------------------------------------------------------------------------------------
\end{verbatim}

In the presence of interaction terms in the model, the predictors are
scaled and centered before computing the standardized betas.
\texttt{ols\_regress()} will detect interaction terms automatically but
in case you have created a new variable instead of using the inline
function \texttt{*}, you can indicate the presence of interaction terms
by setting \texttt{iterm} to \texttt{TRUE}.

\hypertarget{residual-vs-fitted-values-plot}{%
\subsubsection{Residual vs Fitted Values
Plot}\label{residual-vs-fitted-values-plot}}

Plot to detect non-linearity, unequal error variances, and outliers.
Each point is the error in each vector. Red line just marks the 0 to
have a visual benchmark.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_fit}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{r_glm_files/figure-latex/unnamed-chunk-1-1.pdf} \#\#\#
DFBETAs Panel \#\#\# DFBETAs measure the difference in each parameter
estimate with and without the influential observation.
\texttt{dfbetas\_panel} creates plots to detect influential observations
using DFBETAs.

Belsley, Kuh, and Welsch MATH sugirieron una estadística que indica
cuanto el coeficiente de regresión estimado \(b_{i}\) cambia, en
unidades de desviaciones estándar, si la \(i-\acute {e}sima\)
observación fuera eliminada. La estadística es

\[ DFBETAS_{i,j} = \frac{b_{i}-b_{j(i)}}{\sqrt{s^2_{i}C_{jj}}} \] Donde
MATH es la varianza del coeficiente de regresión \(b_{j}\) calculada sin
la \(i-\acute{e}sima\) observación. Un valor grande de DFBETAS\(_{j,i}\)
indica que la \(i-\acute{e}sima\) observación tiene una considerable
influencia sobre el \(j-\acute{e}simo\) coeficiente de regresión
\(b_{j}\).
\href{http://red.unal.edu.co/cursos/ciencias/2007315/html/un6/cont_12_73.html}{reference}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_dfbetas}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{r_glm_files/figure-latex/unnamed-chunk-2-1.pdf}

\hypertarget{residual-fit-spread-plot}{%
\subsubsection{Residual Fit Spread
Plot}\label{residual-fit-spread-plot}}

Plot to detect non-linearity, influential observations and outliers.

Each spread plot is a graph of centered data values plotted against the
estimated cumulative probability. Thus, spread plots are similar to a
(rotated) plot of the empirical cumulative distribution function.
\href{https://blogs.sas.com/content/iml/2013/06/12/interpret-residual-fit-spread-plot.html}{reference}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_fit\_spread}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/rfsplot2-1} \end{center}

\hypertarget{breusch-pagan-test}{%
\subsubsection{Breusch Pagan Test}\label{breusch-pagan-test}}

Breusch Pagan test is used to test for herteroskedasticity (non-constant
error variance). It tests whether the variance of the errors from a
regression is dependent on the values of the independent variables. It
is a \(\chi^{2}\) test.

Null hypothesis implies the variance is constant and using an alpha of
0.05 then in this case we reject the null hypothesis because p-value is
0.23

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ drat, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_breusch\_pagan}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Breusch Pagan Test for Heteroskedasticity
##  -----------------------------------------
##  Ho: the variance is constant            
##  Ha: the variance is not constant        
## 
##              Data               
##  -------------------------------
##  Response : mpg 
##  Variables: fitted values of mpg 
## 
##        Test Summary         
##  ---------------------------
##  DF            =    1 
##  Chi2          =    1.429672 
##  Prob > Chi2   =    0.231818
\end{verbatim}

\hypertarget{collinearity-diagnostics}{%
\subsubsection{Collinearity
Diagnostics}\label{collinearity-diagnostics}}

collinearity, in statistics, correlation between predictor variables (or
independent variables), such that they express a linear relationship in
a regression model. When predictor variables in the same regression
model are correlated, they cannot independently predict the value of the
dependent variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_coll\_diag}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Tolerance and Variance Inflation Factor
## ---------------------------------------
##   Variables Tolerance      VIF
## 1      disp 0.1252279 7.985439
## 2        hp 0.1935450 5.166758
## 3        wt 0.1445726 6.916942
## 4      qsec 0.3191708 3.133119
## 
## 
## Eigenvalue and Condition Index
## ------------------------------
##    Eigenvalue Condition Index   intercept        disp          hp           wt
## 1 4.721487187        1.000000 0.000123237 0.001132468 0.001413094 0.0005253393
## 2 0.216562203        4.669260 0.002617424 0.036811051 0.027751289 0.0002096014
## 3 0.050416837        9.677242 0.001656551 0.120881424 0.392366164 0.0377028008
## 4 0.010104757       21.616057 0.025805998 0.777260487 0.059594623 0.7017528428
## 5 0.001429017       57.480524 0.969796790 0.063914571 0.518874831 0.2598094157
##           qsec
## 1 0.0001277169
## 2 0.0046789491
## 3 0.0001952599
## 4 0.0024577686
## 5 0.9925403056
\end{verbatim}

\hypertarget{stepwise-regression}{%
\subsubsection{Stepwise Regression}\label{stepwise-regression}}

Build regression model from a set of candidate predictor variables by
entering and removing predictors based on p values, in a stepwise manner
until there is no variable left to enter or remove any more.

Here p-value and Akaike Information Criterion are used to decide which
model is the best in each step of the algorithm.

\hypertarget{variable-selection}{%
\subsubsection{Variable Selection}\label{variable-selection}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_both\_p}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 
##                                 Stepwise Summary                                
## ------------------------------------------------------------------------------
## Step    Variable             AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------------
##  0      Base Model         802.606    806.584    646.794    0.00000    0.00000 
##  1      liver_test (+)     771.875    777.842    616.009    0.45454    0.44405 
##  2      alc_heavy (+)      761.439    769.395    605.506    0.56674    0.54975 
##  3      enzyme_test (+)    750.509    760.454    595.297    0.65900    0.63854 
##  4      pindex (+)         735.715    747.649    582.943    0.75015    0.72975 
##  5      bcs (+)            730.620    744.543    579.638    0.78091    0.75808 
## ------------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{plot}{%
\subsubsection{Plot}\label{plot}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_both\_p}\NormalTok{(model)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/stepwise21-1} \end{center}

\#\#\#Stepwise AIC Backward Regression

Build regression model from a set of candidate predictor variables by
removing predictors based on Akaike Information Criteria, in a stepwise
manner until there is no variable left to remove any more.

\#\#\#Variable Selection

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise aic backward regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_backward\_aic}\NormalTok{(model)}
\NormalTok{k}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 
##                              Stepwise Summary                              
## -------------------------------------------------------------------------
## Step    Variable        AIC        SBC       SBIC        R2       Adj. R2 
## -------------------------------------------------------------------------
##  0      Full Model    736.390    756.280    586.665    0.78184    0.74305 
##  1      alc_mod       734.407    752.308    583.884    0.78177    0.74856 
##  2      gender        732.494    748.406    581.290    0.78142    0.75351 
##  3      age           730.620    744.543    578.844    0.78091    0.75808 
## -------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{plot-1}{%
\subsubsection{Plot}\label{plot-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_backward\_aic}\NormalTok{(model)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/stepaicb21-1} \end{center}

\hypertarget{variable-selection-methods}{%
\subsection{Variable Selection
Methods}\label{variable-selection-methods}}

\hypertarget{introduction}{%
\subsubsection{Introduction}\label{introduction}}

\hypertarget{all-possible-regression}{%
\subsubsection{All Possible Regression}\label{all-possible-regression}}

All subset regression tests, all possible subsets of the set of
potential independent variables. If there are K potential independent
variables (besides the constant), then there are \(2^{k}\) distinct
subsets of them to be tested. For example, if you have 10 candidate
independent variables, the number of subsets to be tested is \(2^{10}\),
which is 1024, and if you have 20 candidate variables, the number is
\(2^{20}\), which is more than one million.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_step\_all\_possible}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Index N      Predictors  R-Square Adj. R-Square Mallow's Cp
## 3      1 1              wt 0.7528328     0.7445939  0.70869536
## 1      2 1            disp 0.7183433     0.7089548  0.67512054
## 2      3 1              hp 0.6024373     0.5891853  0.50969578
## 4      4 1            qsec 0.1752963     0.1478062  0.07541973
## 8      5 2           hp wt 0.8267855     0.8148396  0.78108710
## 10     6 2         wt qsec 0.8264161     0.8144448  0.77856272
## 6      7 2         disp wt 0.7809306     0.7658223  0.72532105
## 5      8 2         disp hp 0.7482402     0.7308774  0.69454380
## 7      9 2       disp qsec 0.7215598     0.7023571  0.66395284
## 9     10 2         hp qsec 0.6368769     0.6118339  0.52014395
## 14    11 3      hp wt qsec 0.8347678     0.8170643  0.78199548
## 11    12 3      disp hp wt 0.8268361     0.8082829  0.76789526
## 13    13 3    disp wt qsec 0.8264170     0.8078189  0.76988533
## 12    14 3    disp hp qsec 0.7541953     0.7278591  0.68301440
## 15    15 4 disp hp wt qsec 0.8351443     0.8107212  0.77102968
\end{verbatim}

The \texttt{plot} method shows the panel of fit criteria for all
possible regression methods.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_all\_possible}\NormalTok{(model)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/allsubplot-1} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/allsubplot-2} \end{center}

\hypertarget{best-subset-regression}{%
\subsubsection{Best Subset Regression}\label{best-subset-regression}}

Select the subset of predictors that do the best at meeting some
well-defined objective criterion, such as having the largest R2 value or
the smallest MSE, Mallow's Cp or AIC.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_step\_best\_subset}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Best Subsets Regression    
## ------------------------------
## Model Index    Predictors
## ------------------------------
##      1         wt              
##      2         hp wt           
##      3         hp wt qsec      
##      4         disp hp wt qsec 
## ------------------------------
## 
##                                                    Subsets Regression Summary                                                    
## ---------------------------------------------------------------------------------------------------------------------------------
##                        Adj.        Pred                                                                                           
## Model    R-Square    R-Square    R-Square     C(p)        AIC        SBIC        SBC         MSEP       FPE       HSP       APC  
## ---------------------------------------------------------------------------------------------------------------------------------
##   1        0.7528      0.7446      0.7087    12.4809    166.0294    74.2916    170.4266    296.9167    9.8572    0.3199    0.2801 
##   2        0.8268      0.8148      0.7811     2.3690    156.6523    66.5755    162.5153    215.5104    7.3563    0.2402    0.2091 
##   3        0.8348      0.8171       0.782     3.0617    157.1426    67.7238    164.4713    213.1929    7.4756    0.2461    0.2124 
##   4        0.8351      0.8107       0.771     5.0000    159.0696    70.0408    167.8640    220.8882    7.9497    0.2644    0.2259 
## ---------------------------------------------------------------------------------------------------------------------------------
## AIC: Akaike Information Criteria 
##  SBIC: Sawa's Bayesian Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
##  MSEP: Estimated error of prediction, assuming multivariate normality 
##  FPE: Final Prediction Error 
##  HSP: Hocking's Sp 
##  APC: Amemiya Prediction Criteria
\end{verbatim}

The \texttt{plot} method shows the panel of fit criteria for best subset
regression methods.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_best\_subset}\NormalTok{(model)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/bestsubplot-1} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/bestsubplot-2} \end{center}

\hypertarget{stepwise-forward-regression}{%
\subsubsection{Stepwise Forward
Regression}\label{stepwise-forward-regression}}

Build regression model from a set of candidate predictor variables by
entering predictors based on p values, in a stepwise manner until there
is no variable left to enter any more. The model should include all the
candidate predictor variables. If details is set to \texttt{TRUE}, each
step is displayed.

\hypertarget{variable-selection-1}{%
\subparagraph{Variable Selection}\label{variable-selection-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise forward regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_forward\_p}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 
##                               Stepwise Summary                              
## --------------------------------------------------------------------------
## Step    Variable         AIC        SBC       SBIC        R2       Adj. R2 
## --------------------------------------------------------------------------
##  0      Base Model     802.606    806.584    646.794    0.00000    0.00000 
##  1      liver_test     771.875    777.842    616.009    0.45454    0.44405 
##  2      alc_heavy      761.439    769.395    605.506    0.56674    0.54975 
##  3      enzyme_test    750.509    760.454    595.297    0.65900    0.63854 
##  4      pindex         735.715    747.649    582.943    0.75015    0.72975 
##  5      bcs            730.620    744.543    579.638    0.78091    0.75808 
## --------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{plot-2}{%
\paragraph{Plot}\label{plot-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_forward\_p}\NormalTok{(model)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/stepf2-1} \end{center}

\hypertarget{detailed-output}{%
\paragraph{Detailed Output}\label{detailed-output}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise forward regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_forward\_p}\NormalTok{(model, }\AttributeTok{details =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Forward Selection Method 
## ------------------------
## 
## Candidate Terms: 
## 
## 1. bcs 
## 2. pindex 
## 3. enzyme_test 
## 4. liver_test 
## 5. age 
## 6. gender 
## 7. alc_mod 
## 8. alc_heavy 
## 
## 
## Step   => 0 
## Model  => y ~ 1 
## R2     => 0 
## 
## Initiating stepwise selection... 
## 
##                      Selection Metrics Table                      
## -----------------------------------------------------------------
## Predictor      Pr(>|t|)    R-Squared    Adj. R-Squared      AIC   
## -----------------------------------------------------------------
## liver_test      0.00000        0.455             0.444    771.875 
## enzyme_test     0.00000        0.334             0.322    782.629 
## pindex          0.00155        0.177             0.161    794.100 
## alc_heavy       0.00172        0.174             0.158    794.301 
## bcs             0.01025        0.120             0.103    797.697 
## alc_mod         0.19286        0.032             0.014    802.828 
## gender          0.20972        0.030             0.011    802.956 
## age             0.39073        0.014            -0.005    803.834 
## -----------------------------------------------------------------
## 
## Step      => 1 
## Selected  => liver_test 
## Model     => y ~ liver_test 
## R2        => 0.455 
## 
##                      Selection Metrics Table                      
## -----------------------------------------------------------------
## Predictor      Pr(>|t|)    R-Squared    Adj. R-Squared      AIC   
## -----------------------------------------------------------------
## alc_heavy       0.00065        0.567             0.550    761.439 
## enzyme_test     0.00089        0.562             0.544    762.077 
## pindex          0.07087        0.489             0.469    770.387 
## alc_mod         0.10979        0.481             0.461    771.141 
## gender          0.79395        0.455             0.434    773.802 
## age             0.83908        0.455             0.434    773.831 
## bcs             0.93062        0.455             0.433    773.867 
## -----------------------------------------------------------------
## 
## Step      => 2 
## Selected  => alc_heavy 
## Model     => y ~ liver_test + alc_heavy 
## R2        => 0.567 
## 
##                      Selection Metrics Table                      
## -----------------------------------------------------------------
## Predictor      Pr(>|t|)    R-Squared    Adj. R-Squared      AIC   
## -----------------------------------------------------------------
## enzyme_test     0.00057        0.659             0.639    750.509 
## pindex          0.00961        0.622             0.599    756.125 
## bcs             0.55687        0.570             0.544    763.063 
## age             0.58269        0.569             0.544    763.110 
## alc_mod         0.91757        0.567             0.541    763.428 
## gender          0.93799        0.567             0.541    763.433 
## -----------------------------------------------------------------
## 
## Step      => 3 
## Selected  => enzyme_test 
## Model     => y ~ liver_test + alc_heavy + enzyme_test 
## R2        => 0.659 
## 
##                     Selection Metrics Table                     
## ---------------------------------------------------------------
## Predictor    Pr(>|t|)    R-Squared    Adj. R-Squared      AIC   
## ---------------------------------------------------------------
## pindex          1e-04        0.750             0.730    735.715 
## bcs           0.21294        0.670             0.643    750.782 
## alc_mod       0.75743        0.660             0.632    752.403 
## age           0.77290        0.660             0.632    752.416 
## gender        0.99197        0.659             0.631    752.509 
## ---------------------------------------------------------------
## 
## Step      => 4 
## Selected  => pindex 
## Model     => y ~ liver_test + alc_heavy + enzyme_test + pindex 
## R2        => 0.75 
## 
##                     Selection Metrics Table                     
## ---------------------------------------------------------------
## Predictor    Pr(>|t|)    R-Squared    Adj. R-Squared      AIC   
## ---------------------------------------------------------------
## bcs           0.01248        0.781             0.758    730.620 
## age           0.86220        0.750             0.724    737.680 
## gender        0.96390        0.750             0.724    737.712 
## alc_mod       0.97040        0.750             0.724    737.713 
## ---------------------------------------------------------------
## 
## Step      => 5 
## Selected  => bcs 
## Model     => y ~ liver_test + alc_heavy + enzyme_test + pindex + bcs 
## R2        => 0.781 
## 
##                     Selection Metrics Table                     
## ---------------------------------------------------------------
## Predictor    Pr(>|t|)    R-Squared    Adj. R-Squared      AIC   
## ---------------------------------------------------------------
## age           0.74164        0.781             0.754    732.494 
## gender        0.80666        0.781             0.753    732.551 
## alc_mod       0.94086        0.781             0.753    732.614 
## ---------------------------------------------------------------
## 
## 
## No more variables to be added.
## 
## Variables Selected: 
## 
## => liver_test 
## => alc_heavy 
## => enzyme_test 
## => pindex 
## => bcs
\end{verbatim}

\begin{verbatim}
## 
## 
##                               Stepwise Summary                              
## --------------------------------------------------------------------------
## Step    Variable         AIC        SBC       SBIC        R2       Adj. R2 
## --------------------------------------------------------------------------
##  0      Base Model     802.606    806.584    646.794    0.00000    0.00000 
##  1      liver_test     771.875    777.842    616.009    0.45454    0.44405 
##  2      alc_heavy      761.439    769.395    605.506    0.56674    0.54975 
##  3      enzyme_test    750.509    760.454    595.297    0.65900    0.63854 
##  4      pindex         735.715    747.649    582.943    0.75015    0.72975 
##  5      bcs            730.620    744.543    579.638    0.78091    0.75808 
## --------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{stepwise-backward-regression}{%
\subsubsection{Stepwise Backward
Regression}\label{stepwise-backward-regression}}

Build regression model from a set of candidate predictor variables by
removing predictors based on p values, in a stepwise manner until there
is no variable left to remove any more. The model should include all the
candidate predictor variables. If details is set to \texttt{TRUE}, each
step is displayed.

\hypertarget{variable-selection-2}{%
\paragraph{Variable Selection}\label{variable-selection-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise backward regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_backward\_p}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 
##                              Stepwise Summary                              
## -------------------------------------------------------------------------
## Step    Variable        AIC        SBC       SBIC        R2       Adj. R2 
## -------------------------------------------------------------------------
##  0      Full Model    736.390    756.280    586.665    0.78184    0.74305 
##  1      alc_mod       734.407    752.308    584.276    0.78177    0.74856 
##  2      gender        732.494    748.406    581.938    0.78142    0.75351 
##  3      age           730.620    744.543    579.638    0.78091    0.75808 
## -------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{plot-3}{%
\paragraph{Plot}\label{plot-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_backward\_p}\NormalTok{(model)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/stepb2-1} \end{center}

\hypertarget{detailed-output-1}{%
\paragraph{Detailed Output}\label{detailed-output-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise backward regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_backward\_p}\NormalTok{(model, }\AttributeTok{details =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Backward Elimination Method 
## ---------------------------
## 
## Candidate Terms: 
## 
## 1. bcs 
## 2. pindex 
## 3. enzyme_test 
## 4. liver_test 
## 5. age 
## 6. gender 
## 7. alc_mod 
## 8. alc_heavy 
## 
## 
## Step   => 0 
## Model  => y ~ bcs + pindex + enzyme_test + liver_test + age + gender + alc_mod + alc_heavy 
## R2     => 0.782 
## 
## Initiating stepwise selection... 
## 
## Step     => 1 
## Removed  => alc_mod 
## Model    => y ~ bcs + pindex + enzyme_test + liver_test + age + gender + alc_heavy 
## R2       => 0.78177 
## 
## Step     => 2 
## Removed  => gender 
## Model    => y ~ bcs + pindex + enzyme_test + liver_test + age + alc_heavy 
## R2       => 0.78142 
## 
## Step     => 3 
## Removed  => age 
## Model    => y ~ bcs + pindex + enzyme_test + liver_test + alc_heavy 
## R2       => 0.78091 
## 
## 
## No more variables to be removed.
## 
## Variables Removed: 
## 
## => alc_mod 
## => gender 
## => age
\end{verbatim}

\begin{verbatim}
## 
## 
##                              Stepwise Summary                              
## -------------------------------------------------------------------------
## Step    Variable        AIC        SBC       SBIC        R2       Adj. R2 
## -------------------------------------------------------------------------
##  0      Full Model    736.390    756.280    586.665    0.78184    0.74305 
##  1      alc_mod       734.407    752.308    584.276    0.78177    0.74856 
##  2      gender        732.494    748.406    581.938    0.78142    0.75351 
##  3      age           730.620    744.543    579.638    0.78091    0.75808 
## -------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{stepwise-regression-1}{%
\subsubsection{Stepwise Regression}\label{stepwise-regression-1}}

Build regression model from a set of candidate predictor variables by
entering and removing predictors based on p values, in a stepwise manner
until there is no variable left to enter or remove any more. The model
should include all the candidate predictor variables. If details is set
to \texttt{TRUE}, each step is displayed.

\hypertarget{variable-selection-3}{%
\paragraph{Variable Selection}\label{variable-selection-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_both\_p}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 
##                                 Stepwise Summary                                
## ------------------------------------------------------------------------------
## Step    Variable             AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------------
##  0      Base Model         802.606    806.584    646.794    0.00000    0.00000 
##  1      liver_test (+)     771.875    777.842    616.009    0.45454    0.44405 
##  2      alc_heavy (+)      761.439    769.395    605.506    0.56674    0.54975 
##  3      enzyme_test (+)    750.509    760.454    595.297    0.65900    0.63854 
##  4      pindex (+)         735.715    747.649    582.943    0.75015    0.72975 
##  5      bcs (+)            730.620    744.543    579.638    0.78091    0.75808 
## ------------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{plot-4}{%
\paragraph{Plot}\label{plot-4}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_both\_p}\NormalTok{(model)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/stepwise23-1} \end{center}

\hypertarget{detailed-output-2}{%
\paragraph{Detailed Output}\label{detailed-output-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_both\_p}\NormalTok{(model, }\AttributeTok{details =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Stepwise Selection Method 
## -------------------------
## 
## Candidate Terms: 
## 
## 1. bcs 
## 2. pindex 
## 3. enzyme_test 
## 4. liver_test 
## 5. age 
## 6. gender 
## 7. alc_mod 
## 8. alc_heavy 
## 
## 
## Step   => 0 
## Model  => y ~ 1 
## R2     => 0 
## 
## Initiating stepwise selection... 
## 
## Step      => 1 
## Selected  => liver_test 
## Model     => y ~ liver_test 
## R2        => 0.455 
## 
## Step      => 2 
## Selected  => alc_heavy 
## Model     => y ~ liver_test + alc_heavy 
## R2        => 0.567 
## 
## Step      => 3 
## Selected  => enzyme_test 
## Model     => y ~ liver_test + alc_heavy + enzyme_test 
## R2        => 0.659 
## 
## Step      => 4 
## Selected  => pindex 
## Model     => y ~ liver_test + alc_heavy + enzyme_test + pindex 
## R2        => 0.75 
## 
## Step      => 5 
## Selected  => bcs 
## Model     => y ~ liver_test + alc_heavy + enzyme_test + pindex + bcs 
## R2        => 0.781 
## 
## 
## No more variables to be added or removed.
\end{verbatim}

\begin{verbatim}
## 
## 
##                                 Stepwise Summary                                
## ------------------------------------------------------------------------------
## Step    Variable             AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------------
##  0      Base Model         802.606    806.584    646.794    0.00000    0.00000 
##  1      liver_test (+)     771.875    777.842    616.009    0.45454    0.44405 
##  2      alc_heavy (+)      761.439    769.395    605.506    0.56674    0.54975 
##  3      enzyme_test (+)    750.509    760.454    595.297    0.65900    0.63854 
##  4      pindex (+)         735.715    747.649    582.943    0.75015    0.72975 
##  5      bcs (+)            730.620    744.543    579.638    0.78091    0.75808 
## ------------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{stepwise-aic-forward-regression}{%
\subsubsection{Stepwise AIC Forward
Regression}\label{stepwise-aic-forward-regression}}

Build regression model from a set of candidate predictor variables by
entering predictors based on Akaike Information Criteria, in a stepwise
manner until there is no variable left to enter any more.

The model should include all the candidate predictor variables. If
details is set to \texttt{TRUE}, each step is displayed.

\hypertarget{variable-selection-4}{%
\paragraph{Variable Selection}\label{variable-selection-4}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise aic forward regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_forward\_aic}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 
##                               Stepwise Summary                              
## --------------------------------------------------------------------------
## Step    Variable         AIC        SBC       SBIC        R2       Adj. R2 
## --------------------------------------------------------------------------
##  0      Base Model     802.606    806.584    646.794    0.00000    0.00000 
##  1      liver_test     771.875    777.842    616.009    0.45454    0.44405 
##  2      alc_heavy      761.439    769.395    605.506    0.56674    0.54975 
##  3      enzyme_test    750.509    760.454    595.297    0.65900    0.63854 
##  4      pindex         735.715    747.649    582.943    0.75015    0.72975 
##  5      bcs            730.620    744.543    579.638    0.78091    0.75808 
## --------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{plot-5}{%
\paragraph{Plot}\label{plot-5}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_forward\_aic}\NormalTok{(model)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/stepaicf2-1} \end{center}

\hypertarget{detailed-output-3}{%
\paragraph{Detailed Output}\label{detailed-output-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise aic forward regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_forward\_aic}\NormalTok{(model, }\AttributeTok{details =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Forward Selection Method 
## ------------------------
## 
## Candidate Terms: 
## 
## 1. bcs 
## 2. pindex 
## 3. enzyme_test 
## 4. liver_test 
## 5. age 
## 6. gender 
## 7. alc_mod 
## 8. alc_heavy 
## 
## 
## Step     => 0 
## Model    => y ~ 1 
## AIC      => 802.606 
## 
## Initiating stepwise selection... 
## 
##                        Table: Adding New Variables                        
## -------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2  
## -------------------------------------------------------------------------
## liver_test      1    771.875    777.842    616.009    0.45454     0.44405 
## enzyme_test     1    782.629    788.596    626.220    0.33435     0.32154 
## pindex          1    794.100    800.067    637.196    0.17680     0.16097 
## alc_heavy       1    794.301    800.268    637.389    0.17373     0.15784 
## bcs             1    797.697    803.664    640.655    0.12010     0.10318 
## alc_mod         1    802.828    808.795    645.601    0.03239     0.01378 
## gender          1    802.956    808.923    645.725    0.03009     0.01143 
## age             1    803.834    809.801    646.572    0.01420    -0.00476 
## -------------------------------------------------------------------------
## 
## Step     => 1 
## Added    => liver_test 
## Model    => y ~ liver_test 
## AIC      => 771.8753 
## 
##                       Table: Adding New Variables                        
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## alc_heavy       1    761.439    769.395    605.506    0.56674    0.54975 
## enzyme_test     1    762.077    770.033    606.090    0.56159    0.54440 
## pindex          1    770.387    778.343    613.737    0.48866    0.46861 
## alc_mod         1    771.141    779.097    614.435    0.48147    0.46113 
## gender          1    773.802    781.758    616.901    0.45528    0.43391 
## age             1    773.831    781.787    616.928    0.45498    0.43361 
## bcs             1    773.867    781.823    616.961    0.45462    0.43323 
## ------------------------------------------------------------------------
## 
## Step     => 2 
## Added    => alc_heavy 
## Model    => y ~ liver_test + alc_heavy 
## AIC      => 761.4394 
## 
##                       Table: Adding New Variables                        
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## enzyme_test     1    750.509    760.454    595.297    0.65900    0.63854 
## pindex          1    756.125    766.070    600.225    0.62163    0.59892 
## bcs             1    763.063    773.008    606.379    0.56975    0.54394 
## age             1    763.110    773.055    606.421    0.56938    0.54354 
## alc_mod         1    763.428    773.373    606.704    0.56683    0.54084 
## gender          1    763.433    773.378    606.709    0.56679    0.54080 
## ------------------------------------------------------------------------
## 
## Step     => 3 
## Added    => enzyme_test 
## Model    => y ~ liver_test + alc_heavy + enzyme_test 
## AIC      => 750.5089 
## 
##                      Table: Adding New Variables                       
## ----------------------------------------------------------------------
## Predictor    DF      AIC        SBC       SBIC        R2       Adj. R2 
## ----------------------------------------------------------------------
## pindex        1    735.715    747.649    582.943    0.75015    0.72975 
## bcs           1    750.782    762.716    595.377    0.66973    0.64277 
## alc_mod       1    752.403    764.337    596.743    0.65967    0.63189 
## age           1    752.416    764.350    596.755    0.65959    0.63180 
## gender        1    752.509    764.443    596.833    0.65900    0.63116 
## ----------------------------------------------------------------------
## 
## Step     => 4 
## Added    => pindex 
## Model    => y ~ liver_test + alc_heavy + enzyme_test + pindex 
## AIC      => 735.7146 
## 
##                      Table: Adding New Variables                       
## ----------------------------------------------------------------------
## Predictor    DF      AIC        SBC       SBIC        R2       Adj. R2 
## ----------------------------------------------------------------------
## bcs           1    730.620    744.543    579.638    0.78091    0.75808 
## age           1    737.680    751.603    585.012    0.75030    0.72429 
## gender        1    737.712    751.635    585.036    0.75016    0.72413 
## alc_mod       1    737.713    751.636    585.037    0.75015    0.72413 
## ----------------------------------------------------------------------
## 
## Step     => 5 
## Added    => bcs 
## Model    => y ~ liver_test + alc_heavy + enzyme_test + pindex + bcs 
## AIC      => 730.6204 
## 
##                      Table: Adding New Variables                       
## ----------------------------------------------------------------------
## Predictor    DF      AIC        SBC       SBIC        R2       Adj. R2 
## ----------------------------------------------------------------------
## age           1    732.494    748.406    581.938    0.78142    0.75351 
## gender        1    732.551    748.463    581.978    0.78119    0.75325 
## alc_mod       1    732.614    748.526    582.023    0.78093    0.75297 
## ----------------------------------------------------------------------
## 
## 
## No more variables to be added.
## 
## Variables Selected: 
## 
## => liver_test 
## => alc_heavy 
## => enzyme_test 
## => pindex 
## => bcs
\end{verbatim}

\begin{verbatim}
## 
## 
##                               Stepwise Summary                              
## --------------------------------------------------------------------------
## Step    Variable         AIC        SBC       SBIC        R2       Adj. R2 
## --------------------------------------------------------------------------
##  0      Base Model     802.606    806.584    646.794    0.00000    0.00000 
##  1      liver_test     771.875    777.842    616.009    0.45454    0.44405 
##  2      alc_heavy      761.439    769.395    605.506    0.56674    0.54975 
##  3      enzyme_test    750.509    760.454    595.297    0.65900    0.63854 
##  4      pindex         735.715    747.649    582.943    0.75015    0.72975 
##  5      bcs            730.620    744.543    579.638    0.78091    0.75808 
## --------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{stepwise-aic-backward-regression}{%
\subsubsection{Stepwise AIC Backward
Regression}\label{stepwise-aic-backward-regression}}

Build regression model from a set of candidate predictor variables by
removing predictors based on Akaike Information Criteria, in a stepwise
manner until there is no variable left to remove any more. The model
should include all the candidate predictor variables. If details is set
to \texttt{TRUE}, each step is displayed.

\hypertarget{variable-selection-5}{%
\paragraph{Variable Selection}\label{variable-selection-5}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise aic backward regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_backward\_aic}\NormalTok{(model)}
\NormalTok{k}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 
##                              Stepwise Summary                              
## -------------------------------------------------------------------------
## Step    Variable        AIC        SBC       SBIC        R2       Adj. R2 
## -------------------------------------------------------------------------
##  0      Full Model    736.390    756.280    586.665    0.78184    0.74305 
##  1      alc_mod       734.407    752.308    583.884    0.78177    0.74856 
##  2      gender        732.494    748.406    581.290    0.78142    0.75351 
##  3      age           730.620    744.543    578.844    0.78091    0.75808 
## -------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{plot-6}{%
\paragraph{Plot}\label{plot-6}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_backward\_aic}\NormalTok{(model)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/stepaicb22-1} \end{center}

\hypertarget{detailed-output-4}{%
\paragraph{Detailed Output}\label{detailed-output-4}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise aic backward regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_backward\_aic}\NormalTok{(model, }\AttributeTok{details =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Backward Elimination Method 
## ---------------------------
## 
## Candidate Terms: 
## 
## 1. bcs 
## 2. pindex 
## 3. enzyme_test 
## 4. liver_test 
## 5. age 
## 6. gender 
## 7. alc_mod 
## 8. alc_heavy 
## 
## 
## Step     => 0 
## Model    => y ~ bcs + pindex + enzyme_test + liver_test + age + gender + alc_mod + alc_heavy 
## AIC      => 736.3899 
## 
## Initiating stepwise selection... 
## 
##                    Table: Removing Existing Variables                    
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## alc_mod         1    734.407    752.308    584.276    0.78177    0.74856 
## gender          1    734.478    752.379    584.323    0.78148    0.74823 
## age             1    734.544    752.445    584.367    0.78121    0.74792 
## liver_test      1    735.878    753.779    585.255    0.77574    0.74162 
## bcs             1    741.677    759.577    589.203    0.75032    0.71233 
## alc_heavy       1    749.210    767.111    594.541    0.71294    0.66926 
## pindex          1    756.624    774.525    600.014    0.67070    0.62059 
## enzyme_test     1    763.557    781.458    605.318    0.62559    0.56861 
## ------------------------------------------------------------------------
## 
## Step     => 1 
## Removed  => alc_mod 
## Model    => y ~ bcs + pindex + enzyme_test + liver_test + age + gender + alc_heavy 
## AIC      => 734.4068 
## 
##                    Table: Removing Existing Variables                    
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## gender          1    732.494    748.406    581.938    0.78142    0.75351 
## age             1    732.551    748.463    581.978    0.78119    0.75325 
## liver_test      1    733.921    749.833    582.951    0.77556    0.74691 
## bcs             1    739.677    755.589    587.106    0.75032    0.71845 
## alc_heavy       1    750.486    766.398    595.217    0.69499    0.65605 
## pindex          1    754.759    770.671    598.530    0.66987    0.62773 
## enzyme_test     1    761.595    777.507    603.950    0.62532    0.57749 
## ------------------------------------------------------------------------
## 
## Step     => 2 
## Removed  => gender 
## Model    => y ~ bcs + pindex + enzyme_test + liver_test + age + alc_heavy 
## AIC      => 732.4942 
## 
##                    Table: Removing Existing Variables                    
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## age             1    730.620    744.543    579.638    0.78091    0.75808 
## liver_test      1    732.339    746.262    580.934    0.77382    0.75026 
## bcs             1    737.680    751.603    585.012    0.75030    0.72429 
## alc_heavy       1    748.486    762.409    593.500    0.69499    0.66322 
## pindex          1    752.777    766.700    596.959    0.66976    0.63536 
## enzyme_test     1    759.596    773.518    602.553    0.62532    0.58629 
## ------------------------------------------------------------------------
## 
## Step     => 3 
## Removed  => age 
## Model    => y ~ bcs + pindex + enzyme_test + liver_test + alc_heavy 
## AIC      => 730.6204 
## 
##                    Table: Removing Existing Variables                    
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## liver_test      1    730.924    742.858    579.087    0.77136    0.75269 
## bcs             1    735.715    747.649    582.943    0.75015    0.72975 
## alc_heavy       1    747.181    759.114    592.362    0.69104    0.66582 
## pindex          1    750.782    762.716    595.377    0.66973    0.64277 
## enzyme_test     1    757.971    769.905    601.477    0.62270    0.59190 
## ------------------------------------------------------------------------
## 
## 
## No more variables to be removed.
## 
## Variables Removed: 
## 
## => alc_mod 
## => gender 
## => age
\end{verbatim}

\begin{verbatim}
## 
## 
##                              Stepwise Summary                              
## -------------------------------------------------------------------------
## Step    Variable        AIC        SBC       SBIC        R2       Adj. R2 
## -------------------------------------------------------------------------
##  0      Full Model    736.390    756.280    586.665    0.78184    0.74305 
##  1      alc_mod       734.407    752.308    583.884    0.78177    0.74856 
##  2      gender        732.494    748.406    581.290    0.78142    0.75351 
##  3      age           730.620    744.543    578.844    0.78091    0.75808 
## -------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{stepwise-aic-regression}{%
\subsubsection{Stepwise AIC Regression}\label{stepwise-aic-regression}}

Build regression model from a set of candidate predictor variables by
entering and removing predictors based on Akaike Information Criteria,
in a stepwise manner until there is no variable left to enter or remove
any more. The model should include all the candidate predictor
variables. If details is set to \texttt{TRUE}, each step is displayed.

\hypertarget{variable-selection-6}{%
\paragraph{Variable Selection}\label{variable-selection-6}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise aic regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_both\_aic}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 
##                                 Stepwise Summary                                
## ------------------------------------------------------------------------------
## Step    Variable             AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------------
##  0      Base Model         802.606    806.584    646.794    0.00000    0.00000 
##  1      liver_test (+)     771.875    777.842    616.009    0.45454    0.44405 
##  2      alc_heavy (+)      761.439    769.395    605.506    0.56674    0.54975 
##  3      enzyme_test (+)    750.509    760.454    595.297    0.65900    0.63854 
##  4      pindex (+)         735.715    747.649    582.943    0.75015    0.72975 
##  5      bcs (+)            730.620    744.543    579.638    0.78091    0.75808 
## ------------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{plot-7}{%
\paragraph{Plot}\label{plot-7}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{ols\_step\_both\_aic}\NormalTok{(model)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/stepwiseaic2-1} \end{center}

\hypertarget{detailed-output-5}{%
\paragraph{Detailed Output}\label{detailed-output-5}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stepwise aic regression}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ surgical)}
\FunctionTok{ols\_step\_both\_aic}\NormalTok{(model, }\AttributeTok{details =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Stepwise Selection Method 
## -------------------------
## 
## Candidate Terms: 
## 
## 1. bcs 
## 2. pindex 
## 3. enzyme_test 
## 4. liver_test 
## 5. age 
## 6. gender 
## 7. alc_mod 
## 8. alc_heavy 
## 
## 
## Step     => 0 
## Model    => y ~ 1 
## AIC      => 802.606 
## 
## Initiating stepwise selection... 
## 
##                        Table: Adding New Variables                        
## -------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2  
## -------------------------------------------------------------------------
## bcs             1    797.697    803.664    640.655    0.12010     0.10318 
## pindex          1    794.100    800.067    637.196    0.17680     0.16097 
## enzyme_test     1    782.629    788.596    626.220    0.33435     0.32154 
## liver_test      1    771.875    777.842    616.009    0.45454     0.44405 
## age             1    803.834    809.801    646.572    0.01420    -0.00476 
## gender          1    802.956    808.923    645.725    0.03009     0.01143 
## alc_mod         1    802.828    808.795    645.601    0.03239     0.01378 
## alc_heavy       1    794.301    800.268    637.389    0.17373     0.15784 
## -------------------------------------------------------------------------
## 
## Step     => 1 
## Added    => liver_test 
## Model    => y ~ liver_test 
## AIC      => 771.8753 
## 
##                       Table: Adding New Variables                        
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## bcs             1    773.867    781.823    616.961    0.45462    0.43323 
## pindex          1    770.387    778.343    613.737    0.48866    0.46861 
## enzyme_test     1    762.077    770.033    606.090    0.56159    0.54440 
## age             1    773.831    781.787    616.928    0.45498    0.43361 
## gender          1    773.802    781.758    616.901    0.45528    0.43391 
## alc_mod         1    771.141    779.097    614.435    0.48147    0.46113 
## alc_heavy       1    761.439    769.395    605.506    0.56674    0.54975 
## ------------------------------------------------------------------------
## 
## Step     => 2 
## Added    => alc_heavy 
## Model    => y ~ liver_test + alc_heavy 
## AIC      => 761.4394 
## 
##                   Table: Removing Existing Variables                    
## -----------------------------------------------------------------------
## Predictor     DF      AIC        SBC       SBIC        R2       Adj. R2 
## -----------------------------------------------------------------------
## liver_test     1    794.301    800.268    637.389    0.17373    0.15784 
## alc_heavy      1    771.875    777.842    616.009    0.45454    0.44405 
## -----------------------------------------------------------------------
## 
##                       Table: Adding New Variables                        
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## bcs             1    763.063    773.008    606.379    0.56975    0.54394 
## pindex          1    756.125    766.070    600.225    0.62163    0.59892 
## enzyme_test     1    750.509    760.454    595.297    0.65900    0.63854 
## age             1    763.110    773.055    606.421    0.56938    0.54354 
## gender          1    763.433    773.378    606.709    0.56679    0.54080 
## alc_mod         1    763.428    773.373    606.704    0.56683    0.54084 
## ------------------------------------------------------------------------
## 
## Step     => 3 
## Added    => enzyme_test 
## Model    => y ~ liver_test + alc_heavy + enzyme_test 
## AIC      => 750.5089 
## 
##                    Table: Removing Existing Variables                    
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## liver_test      1    773.555    781.511    616.671    0.45777    0.43650 
## alc_heavy       1    762.077    770.033    606.090    0.56159    0.54440 
## enzyme_test     1    761.439    769.395    605.506    0.56674    0.54975 
## ------------------------------------------------------------------------
## 
##                      Table: Adding New Variables                       
## ----------------------------------------------------------------------
## Predictor    DF      AIC        SBC       SBIC        R2       Adj. R2 
## ----------------------------------------------------------------------
## bcs           1    750.782    762.716    595.377    0.66973    0.64277 
## pindex        1    735.715    747.649    582.943    0.75015    0.72975 
## age           1    752.416    764.350    596.755    0.65959    0.63180 
## gender        1    752.509    764.443    596.833    0.65900    0.63116 
## alc_mod       1    752.403    764.337    596.743    0.65967    0.63189 
## ----------------------------------------------------------------------
## 
## Step     => 4 
## Added    => pindex 
## Model    => y ~ liver_test + alc_heavy + enzyme_test + pindex 
## AIC      => 735.7146 
## 
##                    Table: Removing Existing Variables                    
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## liver_test      1    748.167    758.112    593.257    0.67347    0.65388 
## alc_heavy       1    755.099    765.044    599.321    0.62875    0.60647 
## enzyme_test     1    756.125    766.070    600.225    0.62163    0.59892 
## pindex          1    750.509    760.454    595.297    0.65900    0.63854 
## ------------------------------------------------------------------------
## 
##                      Table: Adding New Variables                       
## ----------------------------------------------------------------------
## Predictor    DF      AIC        SBC       SBIC        R2       Adj. R2 
## ----------------------------------------------------------------------
## bcs           1    730.620    744.543    579.638    0.78091    0.75808 
## age           1    737.680    751.603    585.012    0.75030    0.72429 
## gender        1    737.712    751.635    585.036    0.75016    0.72413 
## alc_mod       1    737.713    751.636    585.037    0.75015    0.72413 
## ----------------------------------------------------------------------
## 
## Step     => 5 
## Added    => bcs 
## Model    => y ~ liver_test + alc_heavy + enzyme_test + pindex + bcs 
## AIC      => 730.6204 
## 
##                    Table: Removing Existing Variables                    
## ------------------------------------------------------------------------
## Predictor      DF      AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------
## liver_test      1    730.924    742.858    579.087    0.77136    0.75269 
## alc_heavy       1    747.181    759.114    592.362    0.69104    0.66582 
## enzyme_test     1    757.971    769.905    601.477    0.62270    0.59190 
## pindex          1    750.782    762.716    595.377    0.66973    0.64277 
## bcs             1    735.715    747.649    582.943    0.75015    0.72975 
## ------------------------------------------------------------------------
## 
##                      Table: Adding New Variables                       
## ----------------------------------------------------------------------
## Predictor    DF      AIC        SBC       SBIC        R2       Adj. R2 
## ----------------------------------------------------------------------
## age           1    732.494    748.406    581.938    0.78142    0.75351 
## gender        1    732.551    748.463    581.978    0.78119    0.75325 
## alc_mod       1    732.614    748.526    582.023    0.78093    0.75297 
## ----------------------------------------------------------------------
## 
## 
## No more variables to be added or removed.
## 
## Variables Selected: 
## 
## => liver_test 
## => alc_heavy 
## => enzyme_test 
## => pindex 
## => bcs
\end{verbatim}

\begin{verbatim}
## 
## 
##                                 Stepwise Summary                                
## ------------------------------------------------------------------------------
## Step    Variable             AIC        SBC       SBIC        R2       Adj. R2 
## ------------------------------------------------------------------------------
##  0      Base Model         802.606    806.584    646.794    0.00000    0.00000 
##  1      liver_test (+)     771.875    777.842    616.009    0.45454    0.44405 
##  2      alc_heavy (+)      761.439    769.395    605.506    0.56674    0.54975 
##  3      enzyme_test (+)    750.509    760.454    595.297    0.65900    0.63854 
##  4      pindex (+)         735.715    747.649    582.943    0.75015    0.72975 
##  5      bcs (+)            730.620    744.543    579.638    0.78091    0.75808 
## ------------------------------------------------------------------------------
## 
## Final Model Output 
## ------------------
## 
##                            Model Summary                            
## -------------------------------------------------------------------
## R                         0.884       RMSE                 184.276 
## R-Squared                 0.781       MSE                38202.426 
## Adj. R-Squared            0.758       Coef. Var             27.839 
## Pred R-Squared            0.700       AIC                  730.620 
## MAE                     137.656       SBC                  744.543 
## -------------------------------------------------------------------
##  RMSE: Root Mean Square Error 
##  MSE: Mean Square Error 
##  MAE: Mean Absolute Error 
##  AIC: Akaike Information Criteria 
##  SBC: Schwarz Bayesian Criteria 
## 
##                                  ANOVA                                  
## -----------------------------------------------------------------------
##                    Sum of                                              
##                   Squares        DF    Mean Square      F         Sig. 
## -----------------------------------------------------------------------
## Regression    6535804.090         5    1307160.818    34.217    0.0000 
## Residual      1833716.447        48      38202.426                     
## Total         8369520.537        53                                    
## -----------------------------------------------------------------------
## 
##                                       Parameter Estimates                                        
## ------------------------------------------------------------------------------------------------
##       model         Beta    Std. Error    Std. Beta      t        Sig         lower       upper 
## ------------------------------------------------------------------------------------------------
## (Intercept)    -1178.330       208.682                 -5.647    0.000    -1597.914    -758.746 
##  liver_test       58.064        40.144        0.156     1.446    0.155      -22.652     138.779 
##   alc_heavy      317.848        71.634        0.314     4.437    0.000      173.818     461.878 
## enzyme_test        9.748         1.656        0.521     5.887    0.000        6.419      13.077 
##      pindex        8.924         1.808        0.380     4.935    0.000        5.288      12.559 
##         bcs       59.864        23.060        0.241     2.596    0.012       13.498     106.230 
## ------------------------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{notes-on-stepwise}{%
\subsubsection{Notes on stepwise}\label{notes-on-stepwise}}

A fundamental problem with stepwise regression is that some real
explanatory variables that have causal effects on the dependent variable
may happen to not be statistically significant, while nuisance variables
may be coincidentally significant. As a result, the model may fit the
data well in-sample, but do poorly out-of-sample.

Many Big-Data researchers believe that, the larger the number of
possible explanatory variables, the more useful is stepwise regression
for selecting explanatory variables. The reality is that stepwise
regression is less effective the larger the number of potential
explanatory variables. Stepwise regression does not solve the Big-Data
problem of too many explanatory variables. Big Data exacerbates the
failings of stepwise regression.
\href{https://journalofbigdata.springeropen.com/articles/10.1186/s40537-018-0143-6}{reference}

\hypertarget{residual-diagnostics}{%
\subsection{Residual Diagnostics}\label{residual-diagnostics}}

\hypertarget{introduction-1}{%
\subsubsection{Introduction}\label{introduction-1}}

olsrr offers tools for detecting violation of standard regression
assumptions. Here we take a look at residual diagnostics. The standard
regression assumptions include the following about residuals/errors:

\begin{itemize}
\tightlist
\item
  The error has a normal distribution (normality assumption).
\item
  The errors have mean zero.
\item
  The errors have same but unknown variance (homoscedasticity
  assumption).
\item
  The error are independent of each other (independent errors
  assumption).
\end{itemize}

\hypertarget{residual-qq-plot}{%
\subsubsection{Residual QQ Plot}\label{residual-qq-plot}}

Graph for detecting violation of normality assumption.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_qq}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/qqresid-1} \end{center}

\hypertarget{residual-normality-test}{%
\subsubsection{Residual Normality Test}\label{residual-normality-test}}

Test for detecting violation of normality assumption.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_normality}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## -----------------------------------------------
##        Test             Statistic       pvalue  
## -----------------------------------------------
## Shapiro-Wilk              0.9366         0.0600 
## Kolmogorov-Smirnov        0.1152         0.7464 
## Cramer-von Mises          2.8122         0.0000 
## Anderson-Darling          0.5859         0.1188 
## -----------------------------------------------
\end{verbatim}

Correlation between observed residuals and expected residuals under
normality.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_correlation}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.970066
\end{verbatim}

\hypertarget{residual-vs-fitted-values-plot-1}{%
\subsubsection{Residual vs Fitted Values
Plot}\label{residual-vs-fitted-values-plot-1}}

It is a scatter plot of residuals on the y axis and fitted values on the
x axis to detect non-linearity, unequal error variances, and outliers.

\textbf{Characteristics of a well behaved residual vs fitted plot:}

\begin{itemize}
\tightlist
\item
  The residuals spread randomly around the 0 line indicating that the
  relationship is linear.
\item
  The residuals form an approximate horizontal band around the 0 line
  indicating homogeneity of error variance.
\item
  No one residual is visibly away from the random pattern of the
  residuals indicating that there are no outliers.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_fit}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/rvsfplot-1} \end{center}

\hypertarget{residual-histogram}{%
\subsubsection{Residual Histogram}\label{residual-histogram}}

Histogram of residuals for detecting violation of normality assumption.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_hist}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/residhist-1} \end{center}

\hypertarget{heteroscedasticity}{%
\subsection{Heteroscedasticity}\label{heteroscedasticity}}

\hypertarget{introduction-2}{%
\subsubsection{Introduction}\label{introduction-2}}

One of the assumptions made about residuals/errors in OLS regression is
that the errors have the same but unknown variance. This is known as
constant variance or homoscedasticity. When this assumption is violated,
the problem is known as heteroscedasticity.

\hypertarget{consequences-of-heteroscedasticity}{%
\paragraph{Consequences of
Heteroscedasticity}\label{consequences-of-heteroscedasticity}}

\begin{itemize}
\tightlist
\item
  The OLS estimators and regression predictions based on them remains
  unbiased and consistent.
\item
  The OLS estimators are no longer the BLUE (Best Linear Unbiased
  Estimators) because they are no longer efficient, so the regression
  predictions will be inefficient too.
\item
  Because of the inconsistency of the covariance matrix of the estimated
  regression coefficients, the tests of hypotheses, (t-test, F-test) are
  no longer valid.
\end{itemize}

\textbf{olsrr} provides the following 4 tests for detecting
heteroscedasticity:

\begin{itemize}
\tightlist
\item
  Bartlett Test
\item
  Breusch Pagan Test
\item
  Score Test
\item
  F Test
\end{itemize}

\hypertarget{bartlett-test}{%
\subsubsection{Bartlett Test}\label{bartlett-test}}

Bartlett's test is used to test if variances across samples is equal. It
is sensitive to departures from normality. The Levene test is an
alternative test that is less sensitive to departures from normality.

You can perform the test using 2 continuous variables, one continuous
and one grouping variable, a formula or a linear model.

\hypertarget{use-grouping-variable}{%
\paragraph{Use grouping variable}\label{use-grouping-variable}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ols\_test\_bartlett}\NormalTok{(hsb, }\StringTok{\textquotesingle{}read\textquotesingle{}}\NormalTok{, }\AttributeTok{group\_var =} \StringTok{\textquotesingle{}female\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##     Bartlett's Test of Homogenity of Variances    
## ------------------------------------------------
## Ho: Variances are equal across groups
## Ha: Variances are unequal for atleast two groups
## 
##         Test Summary         
##  ----------------------------
##  DF            =    1 
##  Chi2          =    0.1866579 
##  Prob > Chi2   =    0.6657129
\end{verbatim}

\hypertarget{using-variables}{%
\paragraph{Using variables}\label{using-variables}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ols\_test\_bartlett}\NormalTok{(hsb, }\StringTok{\textquotesingle{}read\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}write\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##     Bartlett's Test of Homogenity of Variances    
## ------------------------------------------------
## Ho: Variances are equal across groups
## Ha: Variances are unequal for atleast two groups
## 
##         Data          
##  ---------------------
##  Variables: read write 
## 
##         Test Summary         
##  ----------------------------
##  DF            =    1 
##  Chi2          =    1.222871 
##  Prob > Chi2   =    0.2687979
\end{verbatim}

\hypertarget{breusch-pagan-test-1}{%
\subsubsection{Breusch Pagan Test}\label{breusch-pagan-test-1}}

Breusch Pagan Test was introduced by Trevor Breusch and Adrian Pagan in
1979. It is used to test for heteroskedasticity in a linear regression
model and assumes that the error terms are normally distributed. It
tests whether the variance of the errors from a regression is dependent
on the values of the independent variables. It is a \(\chi^{2}\) test.

You can perform the test using the fitted values of the model, the
predictors in the model and a subset of the independent variables. It
includes options to perform multiple tests and p value adjustments. The
options for p value adjustments include Bonferroni, Sidak and Holm's
method.

\hypertarget{use-fitted-values-of-the-model}{%
\paragraph{Use fitted values of the
model}\label{use-fitted-values-of-the-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ drat, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_breusch\_pagan}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Breusch Pagan Test for Heteroskedasticity
##  -----------------------------------------
##  Ho: the variance is constant            
##  Ha: the variance is not constant        
## 
##              Data               
##  -------------------------------
##  Response : mpg 
##  Variables: fitted values of mpg 
## 
##        Test Summary         
##  ---------------------------
##  DF            =    1 
##  Chi2          =    1.429672 
##  Prob > Chi2   =    0.231818
\end{verbatim}

\hypertarget{use-independent-variables-of-the-model}{%
\paragraph{Use independent variables of the
model}\label{use-independent-variables-of-the-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ drat, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_breusch\_pagan}\NormalTok{(model, }\AttributeTok{rhs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Breusch Pagan Test for Heteroskedasticity
##  -----------------------------------------
##  Ho: the variance is constant            
##  Ha: the variance is not constant        
## 
##            Data            
##  --------------------------
##  Response : mpg 
##  Variables: disp hp wt drat 
## 
##         Test Summary         
##  ----------------------------
##  DF            =    4 
##  Chi2          =    1.513808 
##  Prob > Chi2   =    0.8241927
\end{verbatim}

\hypertarget{use-independent-variables-of-the-model-and-perform-multiple-tests}{%
\paragraph{Use independent variables of the model and perform multiple
tests}\label{use-independent-variables-of-the-model-and-perform-multiple-tests}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ drat, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_breusch\_pagan}\NormalTok{(model, }\AttributeTok{rhs =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{multiple =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Breusch Pagan Test for Heteroskedasticity
##  -----------------------------------------
##  Ho: the variance is constant            
##  Ha: the variance is not constant        
## 
##            Data            
##  --------------------------
##  Response : mpg 
##  Variables: disp hp wt drat 
## 
##         Test Summary (Unadjusted p values)       
##  ----------------------------------------------
##   Variable           chi2       df        p     
##  ----------------------------------------------
##   disp             1.2355345     1    0.2663334 
##   hp               0.9209878     1    0.3372157 
##   wt               1.2529988     1    0.2629805 
##   drat             1.1668486     1    0.2800497 
##  ----------------------------------------------
##   simultaneous     1.5138083     4    0.8241927 
##  ----------------------------------------------
\end{verbatim}

\hypertarget{bonferroni-p-value-adjustment}{%
\paragraph{Bonferroni p value
Adjustment}\label{bonferroni-p-value-adjustment}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ drat, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_breusch\_pagan}\NormalTok{(model, }\AttributeTok{rhs =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{multiple =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{p.adj =} \StringTok{\textquotesingle{}bonferroni\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Breusch Pagan Test for Heteroskedasticity
##  -----------------------------------------
##  Ho: the variance is constant            
##  Ha: the variance is not constant        
## 
##            Data            
##  --------------------------
##  Response : mpg 
##  Variables: disp hp wt drat 
## 
##         Test Summary (Bonferroni p values)       
##  ----------------------------------------------
##   Variable           chi2       df        p     
##  ----------------------------------------------
##   disp             1.2355345     1    1.0000000 
##   hp               0.9209878     1    1.0000000 
##   wt               1.2529988     1    1.0000000 
##   drat             1.1668486     1    1.0000000 
##  ----------------------------------------------
##   simultaneous     1.5138083     4    0.8241927 
##  ----------------------------------------------
\end{verbatim}

\hypertarget{sidak-p-value-adjustment}{%
\paragraph{Sidak p value Adjustment}\label{sidak-p-value-adjustment}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ drat, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_breusch\_pagan}\NormalTok{(model, }\AttributeTok{rhs =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{multiple =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{p.adj =} \StringTok{\textquotesingle{}sidak\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Breusch Pagan Test for Heteroskedasticity
##  -----------------------------------------
##  Ho: the variance is constant            
##  Ha: the variance is not constant        
## 
##            Data            
##  --------------------------
##  Response : mpg 
##  Variables: disp hp wt drat 
## 
##           Test Summary (Sidak p values)          
##  ----------------------------------------------
##   Variable           chi2       df        p     
##  ----------------------------------------------
##   disp             1.2355345     1    0.7102690 
##   hp               0.9209878     1    0.8070305 
##   wt               1.2529988     1    0.7049362 
##   drat             1.1668486     1    0.7313356 
##  ----------------------------------------------
##   simultaneous     1.5138083     4    0.8241927 
##  ----------------------------------------------
\end{verbatim}

\hypertarget{holms-p-value-adjustment}{%
\paragraph{Holm's p value Adjustment}\label{holms-p-value-adjustment}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ drat, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_breusch\_pagan}\NormalTok{(model, }\AttributeTok{rhs =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{multiple =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{p.adj =} \StringTok{\textquotesingle{}holm\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Breusch Pagan Test for Heteroskedasticity
##  -----------------------------------------
##  Ho: the variance is constant            
##  Ha: the variance is not constant        
## 
##            Data            
##  --------------------------
##  Response : mpg 
##  Variables: disp hp wt drat 
## 
##           Test Summary (Holm's p values)         
##  ----------------------------------------------
##   Variable           chi2       df        p     
##  ----------------------------------------------
##   disp             1.2355345     1    0.7990002 
##   hp               0.9209878     1    0.3372157 
##   wt               1.2529988     1    1.0000000 
##   drat             1.1668486     1    0.5600994 
##  ----------------------------------------------
##   simultaneous     1.5138083     4    0.8241927 
##  ----------------------------------------------
\end{verbatim}

\hypertarget{score-test}{%
\subsubsection{Score Test}\label{score-test}}

Test for heteroskedasticity under the assumption that the errors are
independent and identically distributed (i.i.d.). You can perform the
test using the fitted values of the model, the predictors in the model
and a subset of the independent variables.

\hypertarget{use-fitted-values-of-the-model-1}{%
\paragraph{Use fitted values of the
model}\label{use-fitted-values-of-the-model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_score}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Score Test for Heteroskedasticity
##  ---------------------------------
##  Ho: Variance is homogenous
##  Ha: Variance is not homogenous
## 
##  Variables: fitted values of mpg 
## 
##         Test Summary         
##  ----------------------------
##  DF            =    1 
##  Chi2          =    0.5163959 
##  Prob > Chi2   =    0.4723832
\end{verbatim}

\hypertarget{use-independent-variables-of-the-model-1}{%
\paragraph{Use independent variables of the
model}\label{use-independent-variables-of-the-model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_score}\NormalTok{(model, }\AttributeTok{rhs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Score Test for Heteroskedasticity
##  ---------------------------------
##  Ho: Variance is homogenous
##  Ha: Variance is not homogenous
## 
##  Variables: disp hp wt qsec 
## 
##         Test Summary         
##  ----------------------------
##  DF            =    4 
##  Chi2          =    2.039404 
##  Prob > Chi2   =    0.7285114
\end{verbatim}

\hypertarget{specify-variables}{%
\paragraph{Specify variables}\label{specify-variables}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_score}\NormalTok{(model, }\AttributeTok{vars =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}hp\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Score Test for Heteroskedasticity
##  ---------------------------------
##  Ho: Variance is homogenous
##  Ha: Variance is not homogenous
## 
##  Variables: disp hp 
## 
##         Test Summary         
##  ----------------------------
##  DF            =    2 
##  Chi2          =    0.9983196 
##  Prob > Chi2   =    0.6070405
\end{verbatim}

\hypertarget{f-test}{%
\subsubsection{F Test}\label{f-test}}

F Test for heteroskedasticity under the assumption that the errors are
independent and identically distributed (i.i.d.). You can perform the
test using the fitted values of the model, the predictors in the model
and a subset of the independent variables.

\hypertarget{use-fitted-values-of-the-model-2}{%
\paragraph{Use fitted values of the
model}\label{use-fitted-values-of-the-model-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_f}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  F Test for Heteroskedasticity
##  -----------------------------
##  Ho: Variance is homogenous
##  Ha: Variance is not homogenous
## 
##  Variables: fitted values of mpg 
## 
##       Test Summary        
##  -------------------------
##  Num DF     =    1 
##  Den DF     =    30 
##  F          =    0.4920617 
##  Prob > F   =    0.4884154
\end{verbatim}

\hypertarget{use-independent-variables-of-the-model-2}{%
\paragraph{Use independent variables of the
model}\label{use-independent-variables-of-the-model-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_f}\NormalTok{(model, }\AttributeTok{rhs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  F Test for Heteroskedasticity
##  -----------------------------
##  Ho: Variance is homogenous
##  Ha: Variance is not homogenous
## 
##  Variables: disp hp wt qsec 
## 
##       Test Summary        
##  -------------------------
##  Num DF     =    4 
##  Den DF     =    27 
##  F          =    0.4594694 
##  Prob > F   =    0.7647271
\end{verbatim}

\hypertarget{specify-variables-1}{%
\paragraph{Specify variables}\label{specify-variables-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_test\_f}\NormalTok{(model, }\AttributeTok{vars =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}hp\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  F Test for Heteroskedasticity
##  -----------------------------
##  Ho: Variance is homogenous
##  Ha: Variance is not homogenous
## 
##  Variables: disp hp 
## 
##       Test Summary        
##  -------------------------
##  Num DF     =    2 
##  Den DF     =    29 
##  F          =    0.4669306 
##  Prob > F   =    0.631555
\end{verbatim}

\hypertarget{measures-of-influence}{%
\subsection{Measures of Influence}\label{measures-of-influence}}

\hypertarget{introduction-3}{%
\subsubsection{Introduction}\label{introduction-3}}

It is possible for a single observation to have a great influence on the
results of a regression analysis. It is therefore important to detect
influential observations and to take them into consideration when
interpreting the results.

\textbf{olsrr} offers the following tools to detect influential
observations:

\begin{itemize}
\tightlist
\item
  Cook's D Bar Plot
\item
  Cook's D Chart
\item
  DFBETAs Panel
\item
  DFFITs Plot
\item
  Studentized Residual Plot
\item
  Standardized Residual Chart
\item
  Studentized Residuals vs Leverage Plot
\item
  Deleted Studentized Residual vs Fitted Values Plot
\item
  Hadi Plot
\item
  Potential Residual Plot
\end{itemize}

\hypertarget{cooks-d-bar-plot}{%
\paragraph{Cook's D Bar Plot}\label{cooks-d-bar-plot}}

Bar Plot of Cook's distance to detect observations that strongly
influence fitted values of the model. Cook's distance was introduced by
American statistician R Dennis Cook in 1977. It is used to identify
influential data points. It depends on both the residual and leverage
i.e it takes it account both the \textbf{x} value and \textbf{y} value
of the observation.

\textbf{Steps to compute Cook's distance:}

\begin{itemize}
\tightlist
\item
  delete observations one at a time.
\item
  refit the regression model on remaining \((n - 1)\) observations
\item
  examine how much all of the fitted values change when the ith
  observation is deleted.
\end{itemize}

A data point having a large cook's d indicates that the data point
strongly influences the fitted values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_cooksd\_bar}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/ckdbp-1} \end{center}

\hypertarget{cooks-d-chart}{%
\subsubsection{Cook's D Chart}\label{cooks-d-chart}}

Chart of Cook's distance to detect observations that strongly influence
fitted values of the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_cooksd\_chart}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/ckchart-1} \end{center}

\hypertarget{dfbetas-panel}{%
\subsubsection{DFBETAs Panel}\label{dfbetas-panel}}

DFBETA measures the difference in each parameter estimate with and
without the influential point. There is a DFBETA for each data point i.e
if there are n observations and k variables, there will be \(n * k\)
DFBETAs. In general, large values of DFBETAS indicate observations that
are influential in estimating a given parameter. Belsley, Kuh, and
Welsch recommend 2 as a general cutoff value to indicate influential
observations and \(\frac{2}{\sqrt{n}}\) as a size-adjusted cutoff.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_dfbetas}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/dfbpanel-1} \end{center}

\hypertarget{dffits-plot}{%
\subsubsection{DFFITS Plot}\label{dffits-plot}}

Proposed by Welsch and Kuh (1977). It is the scaled difference between
the \(i^{th}\) fitted value obtained from the full data and the
\(i^{th}\) fitted value obtained by deleting the \(i^{th}\) observation.
DFFIT - difference in fits, is used to identify influential data points.
It quantifies the number of standard deviations that the fitted value
changes when the ith data point is omitted.

\textbf{Steps to compute DFFITs:}

\begin{itemize}
\tightlist
\item
  delete observations one at a time.
\item
  refit the regression model on remaining \eqn{n - 1} observations
\item
  examine how much all of the fitted values change when the ith
  observation is deleted.
\end{itemize}

An observation is deemed influential if the absolute value of its DFFITS
value is greater than:

\[{2}*{\frac{\sqrt{(p + 1)}}{(n - p -1)}}\]

where n is the number of observations and p is the number of predictors
including intercept.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_dffits}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/dfitsplot-1} \end{center}

\hypertarget{studentized-residual-plot}{%
\subsubsection{Studentized Residual
Plot}\label{studentized-residual-plot}}

Plot for detecting outliers. Studentized deleted residuals (or
externally studentized residuals) is the deleted residual divided by its
estimated standard deviation. Studentized residuals are going to be more
effective for detecting outlying Y observations than standardized
residuals. If an observation has an externally studentized residual that
is larger than 3 (in absolute value) we can call it an outlier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_stud}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/srplot-1} \end{center}

\hypertarget{standardized-residual-chart}{%
\subsubsection{Standardized Residual
Chart}\label{standardized-residual-chart}}

Chart for detecting outliers. Standardized residual (internally
studentized) is the residual divided by estimated standard deviation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_stand}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/srchart-1} \end{center}

\hypertarget{studentized-residuals-vs-leverage-plot}{%
\subsubsection{Studentized Residuals vs Leverage
Plot}\label{studentized-residuals-vs-leverage-plot}}

Graph for detecting influential observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ write }\SpecialCharTok{+}\NormalTok{ math }\SpecialCharTok{+}\NormalTok{ science, }\AttributeTok{data =}\NormalTok{ hsb)}
\FunctionTok{ols\_plot\_resid\_lev}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/studlev-1} \end{center}

\hypertarget{deleted-studentized-residual-vs-fitted-values-plot}{%
\subsubsection{Deleted Studentized Residual vs Fitted Values
Plot}\label{deleted-studentized-residual-vs-fitted-values-plot}}

Graph for detecting outliers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_stud\_fit}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/dsrvsp-1} \end{center}

\hypertarget{hadi-plot}{%
\subsubsection{Hadi Plot}\label{hadi-plot}}

Hadi's measure of influence based on the fact that influential
observations can be present in either the response variable or in the
predictors or both. The plot is used to detect influential observations
based on Hadi's measure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_hadi}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/hadiplot-1} \end{center}

\hypertarget{potential-residual-plot}{%
\subsubsection{Potential Residual Plot}\label{potential-residual-plot}}

Plot to aid in classifying unusual observations as high-leverage points,
outliers, or a combination of both.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_pot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/potres-1} \end{center}

\hypertarget{collinearity-diagnostics-model-fit-variable-contribution}{%
\subsection{Collinearity Diagnostics, Model Fit \& Variable
Contribution}\label{collinearity-diagnostics-model-fit-variable-contribution}}

\hypertarget{collinearity-diagnostics-1}{%
\subsubsection{Collinearity
Diagnostics}\label{collinearity-diagnostics-1}}

Collinearity implies two variables are near perfect linear combinations
of one another. Multicollinearity involves more than two variables. In
the presence of multicollinearity, regression estimates are unstable and
have high standard errors.

\hypertarget{vif}{%
\paragraph{VIF}\label{vif}}

Variance inflation factors measure the inflation in the variances of the
parameter estimates due to collinearities that exist among the
predictors. It is a measure of how much the variance of the estimated
regression coefficient \(\beta_{k}\) is ``inflated'' by the existence of
correlation among the predictor variables in the model. A VIF of 1 means
that there is no correlation among the kth predictor and the remaining
predictor variables, and hence the variance of \(\beta_{k}\) is not
inflated at all. The general rule of thumb is that VIFs exceeding 4
warrant further investigation, while VIFs exceeding 10 are signs of
serious multicollinearity requiring correction.

Steps to calculate VIF:

\begin{itemize}
\tightlist
\item
  Regress the \(k^{th}\) predictor on rest of the predictors in the
  model.
\item
  Compute the \({R}^{2}_{k}\)
\end{itemize}

\[VIF = \frac{1}{1 - {R}^{2}_{k}} = \frac{1}{Tolerance}\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_vif\_tol}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Variables Tolerance      VIF
## 1      disp 0.1252279 7.985439
## 2        hp 0.1935450 5.166758
## 3        wt 0.1445726 6.916942
## 4      qsec 0.3191708 3.133119
\end{verbatim}

\hypertarget{tolerance}{%
\paragraph{Tolerance}\label{tolerance}}

Percent of variance in the predictor that cannot be accounted for by
other predictors.

Steps to calculate tolerance:

\begin{itemize}
\tightlist
\item
  Regress the \(k^{th}\) predictor on rest of the predictors in the
  model.
\item
  Compute the \({R}^{2}_{k}\)
\end{itemize}

\[Tolerance = 1 - {R}^{2}_{k}\]

\hypertarget{condition-index}{%
\paragraph{Condition Index}\label{condition-index}}

Most multivariate statistical approaches involve decomposing a
correlation matrix into linear combinations of variables. The linear
combinations are chosen so that the first combination has the largest
possible variance (subject to some restrictions we won't discuss), the
second combination has the next largest variance, subject to being
uncorrelated with the first, the third has the largest possible
variance, subject to being uncorrelated with the first and second, and
so forth. The variance of each of these linear combinations is called an
eigenvalue. Collinearity is spotted by finding 2 or more variables that
have large proportions of variance (.50 or more) that correspond to
large condition indices. A rule of thumb is to label as large those
condition indices in the range of 30 or larger.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_eigen\_cindex}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Eigenvalue Condition Index   intercept        disp          hp           wt
## 1 4.721487187        1.000000 0.000123237 0.001132468 0.001413094 0.0005253393
## 2 0.216562203        4.669260 0.002617424 0.036811051 0.027751289 0.0002096014
## 3 0.050416837        9.677242 0.001656551 0.120881424 0.392366164 0.0377028008
## 4 0.010104757       21.616057 0.025805998 0.777260487 0.059594623 0.7017528428
## 5 0.001429017       57.480524 0.969796790 0.063914571 0.518874831 0.2598094157
##           qsec
## 1 0.0001277169
## 2 0.0046789491
## 3 0.0001952599
## 4 0.0024577686
## 5 0.9925403056
\end{verbatim}

\hypertarget{collinearity-diagnostics-2}{%
\paragraph{Collinearity Diagnostics}\label{collinearity-diagnostics-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_coll\_diag}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Tolerance and Variance Inflation Factor
## ---------------------------------------
##   Variables Tolerance      VIF
## 1      disp 0.1252279 7.985439
## 2        hp 0.1935450 5.166758
## 3        wt 0.1445726 6.916942
## 4      qsec 0.3191708 3.133119
## 
## 
## Eigenvalue and Condition Index
## ------------------------------
##    Eigenvalue Condition Index   intercept        disp          hp           wt
## 1 4.721487187        1.000000 0.000123237 0.001132468 0.001413094 0.0005253393
## 2 0.216562203        4.669260 0.002617424 0.036811051 0.027751289 0.0002096014
## 3 0.050416837        9.677242 0.001656551 0.120881424 0.392366164 0.0377028008
## 4 0.010104757       21.616057 0.025805998 0.777260487 0.059594623 0.7017528428
## 5 0.001429017       57.480524 0.969796790 0.063914571 0.518874831 0.2598094157
##           qsec
## 1 0.0001277169
## 2 0.0046789491
## 3 0.0001952599
## 4 0.0024577686
## 5 0.9925403056
\end{verbatim}

\hypertarget{model-fit-assessment}{%
\subsubsection{Model Fit Assessment}\label{model-fit-assessment}}

\hypertarget{residual-fit-spread-plot-1}{%
\paragraph{Residual Fit Spread Plot}\label{residual-fit-spread-plot-1}}

Plot to detect non-linearity, influential observations and outliers.
Consists of side-by-side quantile plots of the centered fit and the
residuals. It shows how much variation in the data is explained by the
fit and how much remains in the residuals. For inappropriate models, the
spread of the residuals in such a plot is often greater than the spread
of the centered fit.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_fit\_spread}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/rfsplot1-1} \end{center}

\hypertarget{part-partial-correlations}{%
\paragraph{Part \& Partial
Correlations}\label{part-partial-correlations}}

\hypertarget{correlations}{%
\subparagraph{Correlations}\label{correlations}}

Relative importance of independent variables in determining \textbf{Y}.
How much each variable uniquely contributes to \(R^{2}\) over and above
that which can be accounted for by the other predictors.

\hypertarget{zero-order}{%
\subparagraph{Zero Order}\label{zero-order}}

Pearson correlation coefficient between the dependent variable and the
independent variables.

\hypertarget{part}{%
\subparagraph{Part}\label{part}}

Unique contribution of independent variables. How much \(R^{2}\) will
decrease if that variable is removed from the model?

\hypertarget{partial}{%
\subparagraph{Partial}\label{partial}}

How much of the variance in \textbf{Y}, which is not estimated by the
other independent variables in the model, is estimated by the specific
variable?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_correlations}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                Correlations                 
## -------------------------------------------
## Variable    Zero Order    Partial     Part  
## -------------------------------------------
## disp            -0.848      0.048     0.019 
## hp              -0.776     -0.224    -0.093 
## wt              -0.868     -0.574    -0.285 
## qsec             0.419      0.219     0.091 
## -------------------------------------------
\end{verbatim}

\hypertarget{observed-vs-predicted-plot}{%
\paragraph{Observed vs Predicted
Plot}\label{observed-vs-predicted-plot}}

Plot of observed vs fitted values to assess the fit of the model.
Ideally, all your points should be close to a regressed diagonal line.
Draw such a diagonal line within your graph and check out where the
points lie. If your model had a high R Square, all the points would be
close to this diagonal line. The lower the R Square, the weaker the
Goodness of fit of your model, the more foggy or dispersed your points
are from this diagonal line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_obs\_fit}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/obspred-1} \end{center}

\hypertarget{lack-of-fit-f-test}{%
\paragraph{Lack of Fit F Test}\label{lack-of-fit-f-test}}

Assess how much of the error in prediction is due to lack of model fit.
The residual sum of squares resulting from a regression can be
decomposed into 2 components:

\begin{itemize}
\tightlist
\item
  Due to lack of fit
\item
  Due to random variation
\end{itemize}

If most of the error is due to lack of fit and not just random error,
the model should be discarded and a new model must be built. The lack of
fit F test works only with simple linear regression. Moreover, it is
important that the data contains repeat observations i.e.~replicates for
at least one of the values of the predictor x. This test generally only
applies to datasets with plenty of replicates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_pure\_error\_anova}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Lack of Fit F Test 
## -----------------
## Response :   mpg 
## Predictor:   disp 
## 
##                       Analysis of Variance Table                       
## ----------------------------------------------------------------------
##                 DF     Sum Sq     Mean Sq     F Value        Pr(>F)    
## ----------------------------------------------------------------------
## disp             1    808.8885    808.8885    314.0095    1.934413e-17 
## Residual        30    317.1587    10.57196                             
##  Lack of fit    25    304.2787    12.17115    4.724824      0.04563623 
##  Pure Error      5       12.88       2.576                             
## ----------------------------------------------------------------------
\end{verbatim}

\hypertarget{diagnostics-panel}{%
\paragraph{Diagnostics Panel}\label{diagnostics-panel}}

Panel of plots for regression diagnostics

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_diagnostics}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-1} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-2} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-3} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-4} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-5} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-6} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-7} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-8} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-9} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-10} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-11} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-12} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/diagpanel-13} \end{center}

\hypertarget{variable-contributions}{%
\subsubsection{Variable Contributions}\label{variable-contributions}}

\hypertarget{residual-vs-regressor-plots}{%
\paragraph{Residual vs Regressor
Plots}\label{residual-vs-regressor-plots}}

Graph to determine whether we should add a new predictor to the model
already containing other predictors. The residuals from the model is
regressed on the new predictor and if the plot shows non random pattern,
you should consider adding the new predictor to the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_resid\_regressor}\NormalTok{(model, }\StringTok{\textquotesingle{}drat\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/rvsrplot-1} \end{center}

\begin{center}\includegraphics{r_glm_files/figure-latex/rvsrplot-2} \end{center}

\hypertarget{added-variable-plot}{%
\paragraph{Added Variable Plot}\label{added-variable-plot}}

Added variable plot provides information about the marginal importance
of a predictor variable \(X_{k}\), given the other predictor variables
already in the model. It shows the marginal importance of the variable
in reducing the residual variability.

The added variable plot was introduced by Mosteller and Tukey (1977). It
enables us to visualize the regression coefficient of a new variable
being considered to be included in a model. The plot can be constructed
for each predictor variable.

Let us assume we want to test the effect of adding/removing variable
\emph{X} from a model. Let the response variable of the model be
\emph{Y}

Steps to construct an added variable plot:

\begin{itemize}
\tightlist
\item
  Regress \emph{Y} on all variables other than \emph{X} and store the
  residuals (\emph{Y} residuals).
\item
  Regress \emph{X} on all the other variables included in the model
  (\emph{X} residuals).
\item
  Construct a scatter plot of \emph{Y} residuals and \emph{X} residuals.
\end{itemize}

What do the \emph{Y} and \emph{X} residuals represent? The \emph{Y}
residuals represent the part of \textbf{Y} not explained by all the
variables other than X. The \emph{X} residuals represent the part of
\textbf{X} not explained by other variables. The slope of the line
fitted to the points in the added variable plot is equal to the
regression coefficient when \textbf{Y} is regressed on all variables
including \textbf{X}.

A strong linear relationship in the added variable plot indicates the
increased importance of the contribution of \textbf{X} to the model
already containing the other predictors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_added\_variable}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
## `geom_smooth()` using formula 'y ~ x'
## `geom_smooth()` using formula 'y ~ x'
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics{r_glm_files/figure-latex/avplot-1} \end{center}

\hypertarget{residual-plus-component-plot}{%
\paragraph{Residual Plus Component
Plot}\label{residual-plus-component-plot}}

The residual plus component plot was introduced by Ezekeil (1924). It
was called as Partial Residual Plot by Larsen and McCleary (1972). Hadi
and Chatterjee (2012) called it the residual plus component plot.

Steps to construct the plot:

\begin{itemize}
\tightlist
\item
  Regress \textbf{Y} on all variables including \textbf{X} and store the
  residuals (\textbf{e}).
\item
  Multiply \textbf{e} with regression coefficient of \textbf{X}
  (\textbf{eX}).
\item
  Construct scatter plot of \textbf{eX} and \textbf{X}
\end{itemize}

The residual plus component plot indicates whether any non-linearity is
present in the relationship between \textbf{Y} and \textbf{X} and can
suggest possible transformationsfor linearizing the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ disp }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ qsec, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{ols\_plot\_comp\_plus\_resid}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
## `geom_smooth()` using formula 'y ~ x'
## `geom_smooth()` using formula 'y ~ x'
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics{r_glm_files/figure-latex/cplusr-1} \end{center}

\hypertarget{a-short-introduction-to-the-blorr-package}{%
\subsection{A Short Introduction to the blorr
Package}\label{a-short-introduction-to-the-blorr-package}}

\hypertarget{introduction-4}{%
\subsubsection{Introduction}\label{introduction-4}}

The blorr package offers tools for building and validating binary
logistic regression models. It is most suitable for
beginner/intermediate R users and those who teach statistics using R.
The APIis very simple and most of the functions take either a
\texttt{data.frame}/\texttt{tibble} or a \texttt{model} as input.
\textbf{blorr} useconsistent prefix \textbf{blr\_} for easy tab
completion.

\hypertarget{installation}{%
\paragraph{Installation}\label{installation}}

You can install \textbf{blorr} using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"blorr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The documentation of the package can be found at
\url{https://blorr.rsquaredacademy.com}. This vignette gives a quick
tour of the package.

\hypertarget{libraries}{%
\subparagraph{Libraries}\label{libraries}}

The following libraries are used in the examples in the vignette:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(blorr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'blorr' was built under R version 4.1.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(magrittr)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data}{%
\paragraph{Data}\label{data}}

To demonstrate the features of blorr, we will use the bank marketing
data set. The data is related with direct marketing campaigns of a
Portuguese banking institution. The marketing campaigns were based on
phone calls. Often, more than one contact to the same client was
required, in order to access if the product (bank term deposit) would be
(`yes') or not (`no') subscribed. It contains a random sample
(\textasciitilde4k) of the original data set which can be found at
\url{https://archive.ics.uci.edu/ml/datasets/bank+marketing}.

\hypertarget{bivariate-analysis}{%
\subsubsection{Bivariate Analysis}\label{bivariate-analysis}}

Let us begin with careful bivariate analysis of each possible variable
and the outcome variable. We will use information value and likelihood
ratio chi square test for selecting the initial set of predictors for
our model. The bivariateanalysis is currently avialable for categorical
predictors only.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_bivariate\_analysis}\NormalTok{(bank\_marketing, y, job, marital, education, default, }
\NormalTok{  housing, loan, contact, poutcome)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                           Bivariate Analysis                           
## ----------------------------------------------------------------------
## Variable     Information Value    LR Chi Square    LR DF    LR p-value 
## ----------------------------------------------------------------------
##    job             0.16              75.2690        11        0.0000   
##  marital           0.05              21.6821         2        0.0000   
## education          0.05              25.0466         3        0.0000   
##  default           0.02              6.0405          1        0.0140   
##  housing           0.16              72.2813         1        0.0000   
##   loan             0.06              26.6615         1        0.0000   
##  contact           0.31             124.3834         2        0.0000   
## poutcome           0.53             270.6450         3        0.0000   
## ----------------------------------------------------------------------
\end{verbatim}

\hypertarget{weight-of-evidence-information-value}{%
\paragraph{Weight of Evidence \& Information
Value}\label{weight-of-evidence-information-value}}

Weight of evidence (WoE) is used to assess the relative risk of diferent
attributes for a characteristic and as a means to transform
characteristics into variables. It is also a very useful tool for
binning. The WoE for any group with average odds is zero. A negative WoE
indicates that the proportion of defaults is higher for that attribute
than the overall proportion and indicates higher risk.

The information value is used to rank order variables in terms of their
predictive power. A high information value indicates a high ability to
discriminate. Values for the information value will always be positive
and may be above 3 when assessing highly predictive characteristics.
Characteristics with information values less than 0:10 are typically
viewed as weak, while values over 0.30 are sought after.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_woe\_iv}\NormalTok{(bank\_marketing, job, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                Weight of Evidence                                
## --------------------------------------------------------------------------------
##    levels        count_0s    count_1s    dist_0s    dist_1s        woe      iv   
## --------------------------------------------------------------------------------
##  management        809         130          0.20       0.25      -0.22     0.01  
##  technician        682          79          0.17       0.15       0.11     0.00  
## entrepreneur       139          12          0.03       0.02       0.40     0.00  
##  blue-collar       937          73          0.23       0.14       0.51     0.05  
##    unknown          29          2           0.01       0.00       0.61     0.00  
##    retired         152          47          0.04       0.09      -0.87     0.05  
##    admin.          433          61          0.11       0.12      -0.09     0.00  
##   services         392          39          0.10       0.08       0.26     0.01  
## self-employed      132          22          0.03       0.04      -0.26     0.00  
##  unemployed        126          15          0.03       0.03       0.08     0.00  
##   housemaid        110          12          0.03       0.02       0.17     0.00  
##    student          63          25          0.02       0.05      -1.13     0.04  
## --------------------------------------------------------------------------------
## 
##       Information Value       
## -----------------------------
## Variable    Information Value 
## -----------------------------
##   job            0.1594       
## -----------------------------
\end{verbatim}

\hypertarget{plot-8}{%
\subparagraph{Plot}\label{plot-8}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{blr\_woe\_iv}\NormalTok{(bank\_marketing, job, y)}
\FunctionTok{plot}\NormalTok{(k)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/woeplot-1} \end{center}

\hypertarget{multiple-variables}{%
\subparagraph{Multiple Variables}\label{multiple-variables}}

We can generate the weight of evidence and information value for
multiple variables using \texttt{blr\_woe\_iv\_stats()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_woe\_iv\_stats}\NormalTok{(bank\_marketing, y, job, marital, education)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Variable: job
## 
##                                Weight of Evidence                                
## --------------------------------------------------------------------------------
##    levels        count_0s    count_1s    dist_0s    dist_1s        woe      iv   
## --------------------------------------------------------------------------------
##  management        809         130          0.20       0.25      -0.22     0.01  
##  technician        682          79          0.17       0.15       0.11     0.00  
## entrepreneur       139          12          0.03       0.02       0.40     0.00  
##  blue-collar       937          73          0.23       0.14       0.51     0.05  
##    unknown          29          2           0.01       0.00       0.61     0.00  
##    retired         152          47          0.04       0.09      -0.87     0.05  
##    admin.          433          61          0.11       0.12      -0.09     0.00  
##   services         392          39          0.10       0.08       0.26     0.01  
## self-employed      132          22          0.03       0.04      -0.26     0.00  
##  unemployed        126          15          0.03       0.03       0.08     0.00  
##   housemaid        110          12          0.03       0.02       0.17     0.00  
##    student          63          25          0.02       0.05      -1.13     0.04  
## --------------------------------------------------------------------------------
## 
##       Information Value       
## -----------------------------
## Variable    Information Value 
## -----------------------------
##   job            0.1594       
## -----------------------------
## 
## 
## Variable: marital
## 
##                             Weight of Evidence                              
## ---------------------------------------------------------------------------
##  levels     count_0s    count_1s    dist_0s    dist_1s        woe      iv   
## ---------------------------------------------------------------------------
## married       2467        273          0.62       0.53       0.15     0.01  
##  single       1079        191          0.27       0.37      -0.32     0.03  
## divorced      458          53          0.11       0.10       0.11     0.00  
## ---------------------------------------------------------------------------
## 
##       Information Value       
## -----------------------------
## Variable    Information Value 
## -----------------------------
## marital          0.0464       
## -----------------------------
## 
## 
## Variable: education
## 
##                              Weight of Evidence                              
## ----------------------------------------------------------------------------
##  levels      count_0s    count_1s    dist_0s    dist_1s        woe      iv   
## ----------------------------------------------------------------------------
## tertiary       1104        195          0.28       0.38      -0.31     0.03  
## secondary      2121        231          0.53       0.45       0.17     0.01  
##  unknown       154          25          0.04       0.05      -0.23     0.00  
##  primary       625          66          0.16       0.13       0.20     0.01  
## ----------------------------------------------------------------------------
## 
##       Information Value        
## ------------------------------
## Variable     Information Value 
## ------------------------------
## education         0.0539       
## ------------------------------
\end{verbatim}

\texttt{blr\_woe\_iv()} and \texttt{blr\_woe\_iv\_stats()} are currently
available for categorical predictors only.

\hypertarget{stepwise-selection}{%
\subsubsection{Stepwise Selection}\label{stepwise-selection}}

For the initial/ first cut model, all the independent variables are put
into the model. Our goal is to include a limited number of independent
variables (5-15) which are all significant, without sacrificing too much
on the model performance. The rationale behind not-including too many
variables is that the model would be over fitted and would become
unstable when tested on the validation sample. The variable reduction is
done using forward or backward or stepwise variable selection
procedures. We will use \texttt{blr\_step\_aic\_both()} to shortlist
predictors for our model.

\hypertarget{model}{%
\paragraph{Model}\label{model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ bank\_marketing, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{\textquotesingle{}logit\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{selection-summary}{%
\subparagraph{Selection Summary}\label{selection-summary}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_step\_aic\_both}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Stepwise Selection Method 
## -------------------------
## 
## Candidate Terms: 
## 
## 1 . age 
## 2 . job 
## 3 . marital 
## 4 . education 
## 5 . default 
## 6 . balance 
## 7 . housing 
## 8 . loan 
## 9 . contact 
## 10 . day 
## 11 . month 
## 12 . duration 
## 13 . campaign 
## 14 . pdays 
## 15 . previous 
## 16 . poutcome 
## 
## 
## Variables Entered/Removed: 
## 
## - duration added 
## - poutcome added 
## - month added 
## - contact added 
## - housing added 
## - loan added 
## - campaign added 
## - marital added 
## - education added 
## - age added 
## 
## No more variables to be added or removed.
\end{verbatim}

\begin{verbatim}
## 
## 
##                      Stepwise Summary                      
## ---------------------------------------------------------
## Variable      Method       AIC         BIC       Deviance 
## ---------------------------------------------------------
## duration     addition    2674.384    2687.217    2670.384 
## poutcome     addition    2396.014    2428.097    2386.014 
## month        addition    2274.109    2376.773    2242.109 
## contact      addition    2207.884    2323.381    2171.884 
## housing      addition    2184.550    2306.463    2146.550 
## loan         addition    2171.972    2300.302    2131.972 
## campaign     addition    2164.164    2298.910    2122.164 
## marital      addition    2158.524    2306.103    2112.524 
## education    addition    2155.837    2322.666    2103.837 
## age          addition    2154.272    2327.517    2100.272 
## ---------------------------------------------------------
\end{verbatim}

\hypertarget{plot-9}{%
\subparagraph{Plot}\label{plot-9}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{blr\_step\_aic\_both}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Stepwise Selection Method 
## -------------------------
## 
## Candidate Terms: 
## 
## 1 . age 
## 2 . job 
## 3 . marital 
## 4 . education 
## 5 . default 
## 6 . balance 
## 7 . housing 
## 8 . loan 
## 9 . contact 
## 10 . day 
## 11 . month 
## 12 . duration 
## 13 . campaign 
## 14 . pdays 
## 15 . previous 
## 16 . poutcome 
## 
## 
## Variables Entered/Removed: 
## 
## - duration added 
## - poutcome added 
## - month added 
## - contact added 
## - housing added 
## - loan added 
## - campaign added 
## - marital added 
## - education added 
## - age added 
## 
## No more variables to be added or removed.
\end{verbatim}

\begin{center}\includegraphics{r_glm_files/figure-latex/stepwise3-1} \end{center}

\hypertarget{regression-output}{%
\subsubsection{Regression Output}\label{regression-output}}

\hypertarget{model-1}{%
\paragraph{Model}\label{model-1}}

We can use bivariate analysis and stepwise selection procedure to
shortlist predictors and build the model using the \texttt{glm()}. The
predictors used in the below model are for illustration purposes and not
necessarily shortlisted from the bivariate analysis and variable
selection procedures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{  age }\SpecialCharTok{+}\NormalTok{ duration }\SpecialCharTok{+}\NormalTok{ previous }\SpecialCharTok{+}\NormalTok{ housing }\SpecialCharTok{+}\NormalTok{ default }\SpecialCharTok{+}
\NormalTok{             loan }\SpecialCharTok{+}\NormalTok{ poutcome }\SpecialCharTok{+}\NormalTok{ job }\SpecialCharTok{+}\NormalTok{ marital, }\AttributeTok{data =}\NormalTok{ bank\_marketing, }
             \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{\textquotesingle{}logit\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Use \texttt{blr\_regress()} to generate comprehensive regression output.
It accepts either of the following

\begin{itemize}
\tightlist
\item
  model built using \texttt{glm()}
\item
  model formula and data
\end{itemize}

\hypertarget{using-model}{%
\subparagraph{Using Model}\label{using-model}}

Let us look at the output generated from \texttt{blr\_regress()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_regress}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                              Model Overview                              
## ------------------------------------------------------------------------
## Data Set    Resp Var    Obs.    Df. Model    Df. Residual    Convergence 
## ------------------------------------------------------------------------
##   data         y        4521      4520           4498           TRUE     
## ------------------------------------------------------------------------
## 
##                     Response Summary                     
## --------------------------------------------------------
## Outcome        Frequency        Outcome        Frequency 
## --------------------------------------------------------
##    0             4004              1              517    
## --------------------------------------------------------
## 
##                      Maximum Likelihood Estimates                       
## -----------------------------------------------------------------------
##    Parameter        DF    Estimate    Std. Error    z value     Pr(>|z|) 
## -----------------------------------------------------------------------
##   (Intercept)       1     -5.1347        0.3728    -13.7729      0.0000 
##       age           1      0.0096        0.0067      1.4299      0.1528 
##     duration        1      0.0042         2e-04     20.7853      0.0000 
##     previous        1     -0.0357        0.0392     -0.9089      0.3634 
##    housingno        1      0.7894        0.1232      6.4098      0.0000 
##    defaultyes       1     -0.8691        0.6919     -1.2562      0.2091 
##      loanno         1      0.6598        0.1945      3.3925       7e-04 
## poutcomefailure     1      0.6085        0.2012      3.0248      0.0025 
##  poutcomeother      1      1.1354        0.2700      4.2057      0.0000 
## poutcomesuccess     1      3.2481        0.2462     13.1913      0.0000 
##  jobtechnician      1     -0.2713        0.1806     -1.5019      0.1331 
## jobentrepreneur     1     -0.7041        0.3809     -1.8486      0.0645 
##  jobblue-collar     1     -0.6132        0.1867     -3.2851      0.0010 
##    jobunknown       1     -0.9932        0.8226     -1.2073      0.2273 
##    jobretired       1      0.3197        0.2729      1.1713      0.2415 
##    jobadmin.        1      0.1120        0.2001      0.5599      0.5755 
##   jobservices       1     -0.1750        0.2265     -0.7728      0.4397 
## jobself-employed    1     -0.1408        0.3009     -0.4680      0.6398 
##  jobunemployed      1     -0.6581        0.3432     -1.9174      0.0552 
##   jobhousemaid      1     -0.7456        0.3932     -1.8963      0.0579 
##    jobstudent       1      0.1927        0.3433      0.5613      0.5746 
##  maritalsingle      1      0.5451        0.1387      3.9299       1e-04 
## maritaldivorced     1     -0.1989        0.1986     -1.0012      0.3167 
## -----------------------------------------------------------------------
## 
##  Association of Predicted Probabilities and Observed Responses  
## ---------------------------------------------------------------
## % Concordant          0.8886          Somers' D        0.7773   
## % Discordant          0.1114          Gamma            0.7773   
## % Tied                0.0000          Tau-a            0.1575   
## Pairs                2070068          c                0.8886   
## ---------------------------------------------------------------
\end{verbatim}

If you want to examine the odds ratio estimates, set
\texttt{odd\_conf\_limit} to \texttt{TRUE}. The odds ratio estimates are
not explicitly computed as we observed considerable increase in
computation time when dealing with large data sets.

\hypertarget{using-formula}{%
\subparagraph{Using Formula}\label{using-formula}}

Let us use the model formula and the data set to generate the above
results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_regress}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{  age }\SpecialCharTok{+}\NormalTok{ duration }\SpecialCharTok{+}\NormalTok{ previous }\SpecialCharTok{+}\NormalTok{ housing }\SpecialCharTok{+}\NormalTok{ default }\SpecialCharTok{+}
\NormalTok{             loan }\SpecialCharTok{+}\NormalTok{ poutcome }\SpecialCharTok{+}\NormalTok{ job }\SpecialCharTok{+}\NormalTok{ marital, }\AttributeTok{data =}\NormalTok{ bank\_marketing)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                              Model Overview                              
## ------------------------------------------------------------------------
## Data Set    Resp Var    Obs.    Df. Model    Df. Residual    Convergence 
## ------------------------------------------------------------------------
##   data         y        4521      4520           4498           TRUE     
## ------------------------------------------------------------------------
## 
##                     Response Summary                     
## --------------------------------------------------------
## Outcome        Frequency        Outcome        Frequency 
## --------------------------------------------------------
##    0             4004              1              517    
## --------------------------------------------------------
## 
##                      Maximum Likelihood Estimates                       
## -----------------------------------------------------------------------
##    Parameter        DF    Estimate    Std. Error    z value     Pr(>|z|) 
## -----------------------------------------------------------------------
##   (Intercept)       1     -5.1347        0.3728    -13.7729      0.0000 
##       age           1      0.0096        0.0067      1.4299      0.1528 
##     duration        1      0.0042         2e-04     20.7853      0.0000 
##     previous        1     -0.0357        0.0392     -0.9089      0.3634 
##    housingno        1      0.7894        0.1232      6.4098      0.0000 
##    defaultyes       1     -0.8691        0.6919     -1.2562      0.2091 
##      loanno         1      0.6598        0.1945      3.3925       7e-04 
## poutcomefailure     1      0.6085        0.2012      3.0248      0.0025 
##  poutcomeother      1      1.1354        0.2700      4.2057      0.0000 
## poutcomesuccess     1      3.2481        0.2462     13.1913      0.0000 
##  jobtechnician      1     -0.2713        0.1806     -1.5019      0.1331 
## jobentrepreneur     1     -0.7041        0.3809     -1.8486      0.0645 
##  jobblue-collar     1     -0.6132        0.1867     -3.2851      0.0010 
##    jobunknown       1     -0.9932        0.8226     -1.2073      0.2273 
##    jobretired       1      0.3197        0.2729      1.1713      0.2415 
##    jobadmin.        1      0.1120        0.2001      0.5599      0.5755 
##   jobservices       1     -0.1750        0.2265     -0.7728      0.4397 
## jobself-employed    1     -0.1408        0.3009     -0.4680      0.6398 
##  jobunemployed      1     -0.6581        0.3432     -1.9174      0.0552 
##   jobhousemaid      1     -0.7456        0.3932     -1.8963      0.0579 
##    jobstudent       1      0.1927        0.3433      0.5613      0.5746 
##  maritalsingle      1      0.5451        0.1387      3.9299       1e-04 
## maritaldivorced     1     -0.1989        0.1986     -1.0012      0.3167 
## -----------------------------------------------------------------------
## 
##  Association of Predicted Probabilities and Observed Responses  
## ---------------------------------------------------------------
## % Concordant          0.8886          Somers' D        0.7773   
## % Discordant          0.1114          Gamma            0.7773   
## % Tied                0.0000          Tau-a            0.1575   
## Pairs                2070068          c                0.8886   
## ---------------------------------------------------------------
\end{verbatim}

\hypertarget{model-fit-statistics}{%
\subsubsection{Model Fit Statistics}\label{model-fit-statistics}}

Model fit statistics are available to assess how well the model fits the
data and to compare two different models.The output includes likelihood
ratio test, AIC, BIC and a host of pseudo r-squared measures. You can
read more about pseudo r-squared at
\url{https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/}.

\hypertarget{single-model}{%
\paragraph{Single Model}\label{single-model}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_model\_fit\_stats}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                Model Fit Statistics                                
## ----------------------------------------------------------------------------------
## Log-Lik Intercept Only:     -1607.330    Log-Lik Full Model:             -1123.340 
## Deviance(4498):              2246.679    LR(22):                           967.980 
##                                          Prob > LR:                          0.000 
## MCFadden's R2                   0.301    McFadden's Adj R2:                  0.287 
## ML (Cox-Snell) R2:              0.193    Cragg-Uhler(Nagelkerke) R2:         0.379 
## McKelvey & Zavoina's R2:        0.388    Efron's R2:                         0.278 
## Count R2:                       0.904    Adj Count R2:                       0.157 
## BIC:                         2440.259    AIC:                             2292.679 
## ----------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{model-validation}{%
\subsubsection{Model Validation}\label{model-validation}}

\hypertarget{confusion-matrix}{%
\paragraph{Confusion Matrix}\label{confusion-matrix}}

In the event of deciding a cut-off point on the probability scores of a
logistic regression model, a confusion matrix is created corresponding
to a particular cut-off. The observations with probability scores above
the cut-off score are predicted to be events and those below the cut-off
score, as non-events. The confusion matrix, a 2X2 table, then calculates
the number of correctly classified and miss-classified observations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_confusion\_matrix}\NormalTok{(model, }\AttributeTok{cutoff =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics 
## 
##           Reference
## Prediction    0    1
##          0 3920  352
##          1   84  165
## 
## 
##                 Accuracy : 0.9036 
##      No Information Rate : 0.8856 
## 
##                    Kappa : 0.3851 
## 
## McNemars's Test P-Value  : 0.0000 
## 
##              Sensitivity : 0.3191 
##              Specificity : 0.9790 
##           Pos Pred Value : 0.6627 
##           Neg Pred Value : 0.9176 
##               Prevalence : 0.1144 
##           Detection Rate : 0.0365 
##     Detection Prevalence : 0.0551 
##        Balanced Accuracy : 0.6491 
##                Precision : 0.6627 
##                   Recall : 0.3191 
## 
##         'Positive' Class : 1
\end{verbatim}

The validity of a cut-off is measured using sensitivity, specificity and
accuracy.

\begin{itemize}
\item
  \textbf{Sensitivity}: The \% of correctly classified events out of all
  events = TP / (TP + FN)
\item
  \textbf{Specificity}: The \% of correctly classified non-events out of
  all non-events = TN / (TN + FP)
\item
  \textbf{Accuracy}: The \% of correctly classified observation over all
  observations = (TP + TN) / (TP + FP + TN + FN)
\item
  \emph{True Positive (TP)} : Events correctly classified as events.
\item
  \emph{True Negative (TN)} : Non-Events correctly classified as
  non-events.
\item
  \emph{False Positive (FP)}: Non-events miss-classified as events.
\item
  \emph{False Negative (FN)}: Events miss-classified as non-events.
\end{itemize}

For a standard logistic model, the higher is the cut-off, the lower will
be the sensitivity and the higher would be the specificity. As the
cut-off is decreased, sensitivity will go up, as then more events would
be captured. Also, specificity will go down, as more non-events would
miss-classified as events. Hence a trade-off is done based on the
requirements. For example, if we are looking to capture as many events
as possible, and we can afford to have miss-classified non-events, then
a low cut-off is taken.

\hypertarget{hosmer-lemeshow-test}{%
\paragraph{Hosmer Lemeshow Test}\label{hosmer-lemeshow-test}}

Hosmer and Lemeshow developed a goodness-of-fit test for logistic
regression models with binary responses. The test involves dividing the
data into approximately ten groups of roughly equal size based on the
percentiles of the estimated probabilities. The observations are sorted
in increasing order of their estimated probability of having an even
outcome. The discrepancies between the observed and expected number of
observations in these groups are summarized by the Pearson chi-square
statistic, which is then compared to chi-square distribution with t
degrees of freedom, where t is the number of groups minus 2. Lower
values of Goodness-of-fit are preferred.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_test\_hosmer\_lemeshow}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            Partition for the Hosmer & Lemeshow Test            
## --------------------------------------------------------------
##                         def = 1                 def = 0        
## Group    Total    Observed    Expected    Observed    Expected 
## --------------------------------------------------------------
##   1       453        2          5.14        451        447.86  
##   2       452        3          8.63        449        443.37  
##   3       452        4         11.88        448        440.12  
##   4       452        7         15.29        445        436.71  
##   5       452        14        19.39        438        432.61  
##   6       452        10        24.97        442        427.03  
##   7       452        31        33.65        421        418.35  
##   8       452        62        49.74        390        402.26  
##   9       452       128        88.10        324        363.90  
##  10       452       256        260.21       196        191.79  
## --------------------------------------------------------------
## 
##      Goodness of Fit Test      
## ------------------------------
## Chi-Square    DF    Pr > ChiSq 
## ------------------------------
##  52.9942      8       0.0000   
## ------------------------------
\end{verbatim}

\hypertarget{gains-table-lift-chart}{%
\paragraph{Gains Table \& Lift Chart}\label{gains-table-lift-chart}}

A lift curve is a graphical representation of the \% of cumulative
events captured at a specific cut-off. The cut-off can be a particular
decile or a percentile. Similar, to rank ordering procedure, the data is
in descending order of the scores and is then grouped into
deciles/percentiles. The cumulative number of observations and events
are then computed for each decile/percentile. The lift curve is the
created using the cumulative \% population as the x-axis and the
cumulative percentage of events as the y-axis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_gains\_table}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    decile total   1   0       ks  tp   tn   fp  fn sensitivity specificity
## 1       1   452 256 196 44.62134 256 3808  196 261    49.51644    95.10490
## 2       2   452 128 324 61.28765 384 3484  520 133    74.27466    87.01299
## 3       3   452  62 390 63.53965 446 3094  910  71    86.26692    77.27273
## 4       4   452  31 421 59.02130 477 2673 1331  40    92.26306    66.75824
## 5       5   452  10 442 49.91657 487 2231 1773  30    94.19729    55.71928
## 6       6   452  14 438 41.68544 501 1793 2211  16    96.90522    44.78022
## 7       7   452   7 445 31.92552 508 1348 2656   9    98.25919    33.66633
## 8       8   452   4 448 21.51040 512  900 3104   5    99.03288    22.47752
## 9       9   452   3 449 10.87689 515  451 3553   2    99.61315    11.26374
## 10     10   453   2 451  0.00000 517    0 4004   0   100.00000     0.00000
##    accuracy
## 1  89.89162
## 2  85.55629
## 3  78.30126
## 4  69.67485
## 5  60.11944
## 6  50.74099
## 7  41.05286
## 8  31.23203
## 9  21.36695
## 10 11.43552
\end{verbatim}

\hypertarget{lift-chart}{%
\subparagraph{Lift Chart}\label{lift-chart}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{blr\_gains\_table}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/val7-1} \end{center}

\hypertarget{roc-curve}{%
\paragraph{ROC Curve}\label{roc-curve}}

ROC curve is a graphical representation of the validity of cut-offs for
a logistic regression model. The ROC curve is plotted using the
sensitivity and specificity for all possible cut-offs, i.e., all the
probability scores. The graph is plotted using sensitivity on the y-axis
and 1-specificity on the x-axis. Any point on the ROC curve represents a
sensitivity X (1-specificity) measure corresponding to a cut-off. The
area under the ROC curve is used as a validation measure for the model
-- the bigger the area the better is the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{blr\_gains\_table}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{blr\_roc\_curve}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/val2-1} \end{center}

\hypertarget{ks-chart}{%
\paragraph{KS Chart}\label{ks-chart}}

The KS Statistic is again a measure of model efficiency, and it is
created using the lift curve. The lift curve is created to plot \%
events. If we also plot \% non-events on the same scale, with \%
population at x-axis, we would get another curve. The maximum distance
between the lift curve for events and that for non-events is termed as
KS. For a good model, KS should be big (\textgreater=0.3) and should
occur as close to the event rate as possible.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{blr\_gains\_table}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{blr\_ks\_chart}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/val3-1} \end{center}

\hypertarget{decile-lift-chart}{%
\paragraph{Decile Lift Chart}\label{decile-lift-chart}}

The decile lift chart displays the lift over the global mean event rate
for each decile. For a model with good discriminatory power, the top
deciles should have an event/conversion rate greater than the global
mean.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{blr\_gains\_table}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{blr\_decile\_lift\_chart}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/val9-1} \end{center}

\hypertarget{capture-rate-by-decile}{%
\paragraph{Capture Rate by Decile}\label{capture-rate-by-decile}}

If the model has good discriminatory power, the top deciles should have
a higher event/conversion rate compared to the bottom deciles.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{blr\_gains\_table}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{blr\_decile\_capture\_rate}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/val8-1} \end{center}

\hypertarget{lorenz-curve}{%
\paragraph{Lorenz Curve}\label{lorenz-curve}}

The Lorenz curve is a simple graphic device which illustrates the degree
of inequality in the distribution of thevariable concerned. It is a
visual representation of inequality used to measure the discriminatory
power of the predictive model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_lorenz\_curve}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/val4-1} \end{center}

\hypertarget{residual-influence-diagnostics}{%
\subsubsection{Residual \& Influence
Diagnostics}\label{residual-influence-diagnostics}}

\textbf{blorr} can generate 22 plots for residual, influence and
leverage diagnostics.

\hypertarget{influence-diagnostics}{%
\subparagraph{Influence Diagnostics}\label{influence-diagnostics}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_plot\_diag\_influence}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/infl-1} \end{center}

\hypertarget{leverage-diagnostics}{%
\subparagraph{Leverage Diagnostics}\label{leverage-diagnostics}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_plot\_diag\_leverage}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/lev-1} \end{center}

\hypertarget{fitted-values-diagnostics}{%
\subparagraph{Fitted Values
Diagnostics}\label{fitted-values-diagnostics}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blr\_plot\_diag\_fit}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_glm_files/figure-latex/fit-1} \end{center}

\end{document}
